---
name: unified-aws-container-gitops
description: |
  Unified, production-ready container deployment to AWS EKS with complete GitOps automation.
  Implements a 4-layer architecture: Infrastructure â†’ Platform â†’ Environments â†’ Applications.
  Consolidates ECR, EKS, ArgoCD, Kustomize, ExternalDNS, Route53, HTTPS/SSL, Terraform IaC,
  and multi-language support (Java, Node.js, Python, Go) into a single comprehensive skill.
  Includes 68 verified fixes, AWS pre-flight capacity checks, idempotent operations with
  restart capability, VPC cleanup scripts, GitOps enforcement guardrails, and comprehensive debugging guides.
  Powered by Opsera - The Unified DevOps Platform.
metadata:
  skillport:
    category: devops
    tags:
    - container
    - docker
    - eks
    - kubernetes
    - terraform
    - gitops
    - argocd
    - kustomize
    - cicd
    - aws
    - ecr
    - route53
    - externaldns
    - https
    - ssl
    - acm
    - java
    - nodejs
    - python
    - go
    - idempotent
    - opsera
    - unified
---

# Unified AWS Container GitOps Skill

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                          â•‘
â•‘    â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘                         â•‘
â•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                         â•‘
â•‘     â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•                         â•‘
â•‘                                                                               â•‘
â•‘                    â”â”â” UNIFIED DEVOPS PLATFORM â”â”â”                           â•‘
â•‘                                                                               â•‘
â•‘          AWS Container GitOps | EKS | ArgoCD | Terraform | Kustomize         â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Version: 3.6.0                           Powered by Opsera                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¤– AGENT BEHAVIOR RULES

**These rules govern how Claude Code should use this skill.**

### Primary Directive
**This skill is the SINGLE SOURCE OF TRUTH for AWS container GitOps deployments.**

### Enforcement Rules

| Rule | Description |
|------|-------------|
| **Skill First** | For ANY deployment question â†’ Use this skill's guidance FIRST |
| **No Mixing** | DO NOT use other deployment skills (opsera-cicd, container-eks-gitops, etc.) |
| **No Improvising** | DO NOT provide deployment advice from general knowledge without consulting this skill |
| **Document Gaps** | If this skill lacks information, document the gap for improvement |

### When This Skill Falls Short

If a user asks something not covered by this skill, respond:

```
"This skill doesn't currently cover [X]. Should I:
  A) Document this as a gap for skill improvement
  B) Provide guidance from other sources (note: may not follow skill standards)"
```

### Gap Documentation Format

When documenting gaps, add to the Changelog section at the end of this file:

```markdown
### Gap Identified: [Date]
- **Scenario**: What the user was trying to do
- **What Was Missing**: What information/guidance was needed
- **Severity**: High/Medium/Low
- **Suggested Addition**: How to improve the skill
```

### ğŸ›¡ï¸ CRITICAL GUARDRAILS

These guardrails MUST be enforced during every deployment session.

#### Guardrail #64: GitOps Violation Detection

**PAUSE before executing ANY kubectl mutating command:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš ï¸  GITOPS VIOLATION DETECTED                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  When user attempts:                                                         â”‚
â”‚    â€¢ kubectl apply -f ...                                                   â”‚
â”‚    â€¢ kubectl set image ...                                                  â”‚
â”‚    â€¢ kubectl patch ...                                                      â”‚
â”‚    â€¢ kubectl delete/create ...                                              â”‚
â”‚                                                                              â”‚
â”‚  PAUSE and ask:                                                              â”‚
â”‚    "âš ï¸ This is a manual kubectl command which bypasses GitOps.              â”‚
â”‚                                                                              â”‚
â”‚     Should I instead:                                                        â”‚
â”‚       A) Update the manifest and commit to Git (GitOps way) âœ…              â”‚
â”‚       B) Proceed with manual kubectl (escape hatch) âš ï¸                      â”‚
â”‚                                                                              â”‚
â”‚     If B: Document WHY GitOps was bypassed for this operation."             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**GitOps Flow (ALWAYS PREFER):**
```
Code Change â†’ Git Commit â†’ GitHub Actions â†’ ECR Push â†’ Update Manifest â†’ Git Push â†’ ArgoCD Sync
```

**Manual kubectl ONLY when:**
- Emergency hotfix (document immediately after)
- Debugging/investigation (read-only commands OK: get, describe, logs)
- Initial cluster bootstrap (before ArgoCD exists)

#### Guardrail #65: Multi-Account Pre-Planning

**VERIFY account consistency BEFORE any deployment:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MULTI-ACCOUNT PRE-FLIGHT CHECK                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Before deployment, verify ALL resources are in SAME account:               â”‚
â”‚                                                                              â”‚
â”‚  $ aws sts get-caller-identity                                              â”‚
â”‚  Account: ____________                                                       â”‚
â”‚                                                                              â”‚
â”‚  Resource Checklist:                                                         â”‚
â”‚  [ ] EKS Cluster account matches?                                           â”‚
â”‚  [ ] ECR Repository account matches?                                        â”‚
â”‚  [ ] ACM Certificate account matches?                                       â”‚
â”‚  [ ] Route53 Hosted Zone account matches? (or document who owns it)        â”‚
â”‚  [ ] IAM Roles/Policies account matches?                                    â”‚
â”‚                                                                              â”‚
â”‚  If ANY mismatch â†’ STOP and create plan for cross-account handling          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Guardrail #66: Preview Before Operating

**ALWAYS preview before applying ANY mutating operation:**

| Tool | Preview Command | Apply Command |
|------|-----------------|---------------|
| **Terraform** | `terraform plan` | `terraform apply` |
| **kubectl** | `kubectl apply --dry-run=client -o yaml` | `kubectl apply` |
| **Kustomize** | `kubectl kustomize overlays/dev` | `kubectl apply -k` |
| **ArgoCD** | `argocd app diff <app>` | `argocd app sync <app>` |
| **AWS CLI** | Add `--dry-run` flag | Remove flag |
| **Git** | `git status && git diff` | `git commit && git push` |

**Agent Behavior:**
```
BEFORE any mutating operation:
1. Show preview of what will change
2. Ask for confirmation
3. Only then execute
4. Log what was applied
```

#### Guardrail #67: Generic AI Fallback Detection

**DETECT when skill cannot help and call it out explicitly:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš ï¸  SKILL GAP DETECTED                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  BEFORE answering any deployment question, check:                           â”‚
â”‚                                                                              â”‚
â”‚  1. Is this scenario covered in the skill?                                  â”‚
â”‚     â€¢ Search skill for relevant section                                     â”‚
â”‚     â€¢ Check fix list for similar issue                                      â”‚
â”‚                                                                              â”‚
â”‚  2. If NOT covered â†’ STOP and declare:                                      â”‚
â”‚                                                                              â”‚
â”‚     "âš ï¸ SKILL GAP DETECTED                                                  â”‚
â”‚                                                                              â”‚
â”‚      This scenario is NOT covered in unified-aws-container-gitops.          â”‚
â”‚      I can provide guidance from general knowledge, but this may            â”‚
â”‚      not follow our established patterns.                                   â”‚
â”‚                                                                              â”‚
â”‚      Gap: [describe what's missing]                                         â”‚
â”‚      Scenario: [what user is trying to do]                                  â”‚
â”‚                                                                              â”‚
â”‚      Options:                                                                â”‚
â”‚        A) Proceed with general AI guidance (may deviate from standards)    â”‚
â”‚        B) Document gap and pause for skill update first                    â”‚
â”‚        C) Find workaround using existing skill patterns"                   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Triggers for Gap Detection:**
- User asks about tool NOT in skill (e.g., Jenkins, GitLab CI)
- User asks about cloud NOT in skill (e.g., GCP, Azure)
- Agent about to suggest command not in skill templates
- Agent finds itself saying "typically" or "generally" (likely using generic AI)

**Feedback Collection:**
When gap is detected, append to Gaps Identified table at end of skill.

---

## Overview

This is the **single source of truth** for deploying containerized applications to AWS EKS using GitOps. It implements a proper **4-layer architecture** that separates concerns and enables true GitOps.

---

## ğŸ”„ STANDARDIZED DEPLOYMENT FLOW

**CRITICAL: Every deployment MUST follow this exact 5-phase flow. No skipping phases.**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     MANDATORY 5-PHASE DEPLOYMENT FLOW                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚  PHASE 1: COLLECT INFORMATION                                           â”‚ â•‘
â•‘  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚ â•‘
â•‘  â”‚  â€¢ Gather all 5 required inputs from user                              â”‚ â•‘
â•‘  â”‚  â€¢ Validate inputs (naming conventions, region, etc.)                  â”‚ â•‘
â•‘  â”‚  â€¢ Create DEPLOYMENT-CONTEXT.md with all parameters                    â”‚ â•‘
â•‘  â”‚  â€¢ DO NOT proceed until all inputs are collected                       â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                    â”‚                                          â•‘
â•‘                                    â–¼                                          â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚  PHASE 2: GENERATE PLAN & USER CONFIRMATION                            â”‚ â•‘
â•‘  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚ â•‘
â•‘  â”‚  â€¢ Generate complete deployment plan                                   â”‚ â•‘
â•‘  â”‚  â€¢ Show EXACTLY what will be created/modified                          â”‚ â•‘
â•‘  â”‚  â€¢ List all resources with names and costs                            â”‚ â•‘
â•‘  â”‚  â€¢ User MUST confirm: "I understand and approve this plan"            â”‚ â•‘
â•‘  â”‚  â€¢ DO NOT proceed without explicit confirmation                        â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                    â”‚                                          â•‘
â•‘                                    â–¼                                          â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚  PHASE 3: PREREQUISITE VALIDATION                                       â”‚ â•‘
â•‘  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚ â•‘
â•‘  â”‚  â€¢ Verify GitHub Actions can run (repo access, workflow exists)        â”‚ â•‘
â•‘  â”‚  â€¢ Verify AWS secrets configured (AWS_ACCESS_KEY_ID, etc.)            â”‚ â•‘
â•‘  â”‚  â€¢ Verify AWS CLI authenticated (aws sts get-caller-identity)         â”‚ â•‘
â•‘  â”‚  â€¢ Verify kubectl configured (if brownfield)                          â”‚ â•‘
â•‘  â”‚  â€¢ Verify required tools installed (terraform, docker, etc.)          â”‚ â•‘
â•‘  â”‚  â€¢ DO NOT proceed if any prerequisite fails                            â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                    â”‚                                          â•‘
â•‘                                    â–¼                                          â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚  PHASE 4: CHECK-IN TO GIT (GitOps)                                      â”‚ â•‘
â•‘  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚ â•‘
â•‘  â”‚  â€¢ Create deployment branch: {app}-{env}                               â”‚ â•‘
â•‘  â”‚  â€¢ Generate all manifests (Terraform, K8s, ArgoCD, GHA workflows)     â”‚ â•‘
â•‘  â”‚  â€¢ Commit to Git with descriptive message                              â”‚ â•‘
â•‘  â”‚  â€¢ Push to remote repository                                           â”‚ â•‘
â•‘  â”‚  â€¢ This is the GitOps trigger - everything flows from Git             â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                    â”‚                                          â•‘
â•‘                                    â–¼                                          â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚  PHASE 5: EXECUTE (Infra â†’ Platform â†’ App)                             â”‚ â•‘
â•‘  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚ â•‘
â•‘  â”‚  â€¢ Step 5a: Infrastructure (Terraform) - VPC, EKS, ECR, IAM           â”‚ â•‘
â•‘  â”‚  â€¢ Step 5b: Platform (if greenfield) - ArgoCD, ExternalDNS            â”‚ â•‘
â•‘  â”‚  â€¢ Step 5c: Application - Docker build, ECR push, ArgoCD sync         â”‚ â•‘
â•‘  â”‚  â€¢ All steps are IDEMPOTENT - safe to re-run                          â”‚ â•‘
â•‘  â”‚  â€¢ Monitor GitHub Actions for execution status                         â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Phase-by-Phase Agent Behavior

#### Phase 1: Collect Information
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AGENT MUST:                                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚  1. Ask for ALL 5 inputs before doing anything else                        â”‚
â”‚  2. Validate each input against naming rules                                â”‚
â”‚  3. Confirm AWS account with: aws sts get-caller-identity                  â”‚
â”‚  4. Create DEPLOYMENT-CONTEXT.md in deployment folder                      â”‚
â”‚  5. Show summary of collected information                                   â”‚
â”‚                                                                             â”‚
â”‚  AGENT MUST NOT:                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  â€¢ Start creating files before all inputs collected                        â”‚
â”‚  â€¢ Assume default values without user confirmation                         â”‚
â”‚  â€¢ Proceed if any required input is missing                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Phase 2: Generate Plan & User Confirmation
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEPLOYMENT PLAN TEMPLATE                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ğŸ“‹ DEPLOYMENT PLAN FOR: {APP_IDENTIFIER}-{ENVIRONMENT}                    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•              â”‚
â”‚                                                                             â”‚
â”‚  WHAT WILL BE CREATED:                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  AWS Resources:                                                             â”‚
â”‚  â–¡ VPC: {tenant}-vpc (if greenfield)                                       â”‚
â”‚  â–¡ EKS Cluster: {tenant}-workload-{env}                                    â”‚
â”‚  â–¡ ECR Repository: {app}-backend                                           â”‚
â”‚  â–¡ ECR Repository: {app}-frontend (if fullstack)                           â”‚
â”‚  â–¡ IAM Roles: ExternalDNS, IRSA                                            â”‚
â”‚                                                                             â”‚
â”‚  Kubernetes Resources:                                                      â”‚
â”‚  â–¡ Namespace: {app}-{env}                                                  â”‚
â”‚  â–¡ Deployment: {app}-backend                                               â”‚
â”‚  â–¡ Service: {app}-backend (LoadBalancer)                                   â”‚
â”‚  â–¡ ArgoCD Application: {app}-argo-{env}                                    â”‚
â”‚                                                                             â”‚
â”‚  Git Resources:                                                             â”‚
â”‚  â–¡ Branch: {app}-{env}                                                     â”‚
â”‚  â–¡ Folder: {app}-opsera/                                                   â”‚
â”‚  â–¡ GitHub Actions workflows                                                 â”‚
â”‚                                                                             â”‚
â”‚  ESTIMATED COSTS (monthly):                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  â€¢ EKS Control Plane: ~$73                                                 â”‚
â”‚  â€¢ EC2 Nodes (t3.large x2): ~$120                                          â”‚
â”‚  â€¢ NAT Gateway: ~$32                                                       â”‚
â”‚  â€¢ LoadBalancer: ~$18                                                      â”‚
â”‚  â€¢ Total estimate: ~$243/month                                             â”‚
â”‚                                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•              â”‚
â”‚  âš ï¸  DO YOU UNDERSTAND AND APPROVE THIS PLAN?                              â”‚
â”‚      Reply: "Yes, I approve" to proceed                                    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Phase 3: Prerequisite Validation
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PREREQUISITE CHECKLIST (Auto-verified)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  GitHub Repository:                                                         â”‚
â”‚  [ ] Repository exists and is accessible                                   â”‚
â”‚  [ ] User has push permissions                                              â”‚
â”‚  [ ] .github/workflows/ directory exists or can be created                 â”‚
â”‚                                                                             â”‚
â”‚  GitHub Secrets (Settings â†’ Secrets â†’ Actions):                            â”‚
â”‚  [ ] AWS_ACCESS_KEY_ID is configured                                       â”‚
â”‚  [ ] AWS_SECRET_ACCESS_KEY is configured                                   â”‚
â”‚  [ ] (Optional) GITHUB_TOKEN has workflow permissions                      â”‚
â”‚                                                                             â”‚
â”‚  AWS CLI:                                                                   â”‚
â”‚  [ ] aws CLI installed: aws --version                                      â”‚
â”‚  [ ] Authenticated: aws sts get-caller-identity                            â”‚
â”‚  [ ] Correct account: {expected_account_id}                                â”‚
â”‚  [ ] Correct region: {expected_region}                                     â”‚
â”‚                                                                             â”‚
â”‚  Required Tools:                                                            â”‚
â”‚  [ ] terraform >= 1.0: terraform --version                                 â”‚
â”‚  [ ] docker: docker --version                                              â”‚
â”‚  [ ] kubectl: kubectl version --client                                     â”‚
â”‚  [ ] git: git --version                                                    â”‚
â”‚                                                                             â”‚
â”‚  If Brownfield Pattern:                                                     â”‚
â”‚  [ ] kubectl context configured for target cluster                         â”‚
â”‚  [ ] ArgoCD accessible (if hub-spoke)                                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Prerequisite Validation Script:**
```bash
#!/bin/bash
# Run this to validate all prerequisites

echo "=== PREREQUISITE VALIDATION ==="

# AWS CLI
echo -n "AWS CLI: "
if aws --version &>/dev/null; then echo "âœ…"; else echo "âŒ Install: brew install awscli"; exit 1; fi

# AWS Authentication
echo -n "AWS Auth: "
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text 2>/dev/null)
if [ -n "$ACCOUNT_ID" ]; then echo "âœ… Account: $ACCOUNT_ID"; else echo "âŒ Run: aws configure"; exit 1; fi

# Terraform
echo -n "Terraform: "
if terraform --version &>/dev/null; then echo "âœ…"; else echo "âŒ Install: brew install terraform"; exit 1; fi

# Docker
echo -n "Docker: "
if docker --version &>/dev/null; then echo "âœ…"; else echo "âŒ Install Docker Desktop"; exit 1; fi

# kubectl
echo -n "kubectl: "
if kubectl version --client &>/dev/null; then echo "âœ…"; else echo "âŒ Install: brew install kubectl"; exit 1; fi

# Git
echo -n "Git: "
if git --version &>/dev/null; then echo "âœ…"; else echo "âŒ Install: brew install git"; exit 1; fi

# GitHub CLI (optional but helpful)
echo -n "GitHub CLI: "
if gh --version &>/dev/null; then echo "âœ…"; else echo "âš ï¸ Optional: brew install gh"; fi

echo ""
echo "=== CHECK GITHUB SECRETS ==="
echo "Verify these secrets exist in your GitHub repo:"
echo "  Settings â†’ Secrets and variables â†’ Actions"
echo "  â–¡ AWS_ACCESS_KEY_ID"
echo "  â–¡ AWS_SECRET_ACCESS_KEY"
echo ""
echo "=== VALIDATION COMPLETE ==="
```

#### Phase 4: Check-in to Git
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GIT OPERATIONS (Automated)                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Step 4.1: Create Branch                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  git checkout -b {app}-{env}                                               â”‚
â”‚                                                                             â”‚
â”‚  Step 4.2: Create Folder Structure                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚  {app}-opsera/                                                              â”‚
â”‚  â”œâ”€â”€ DEPLOYMENT-CONTEXT.md                                                  â”‚
â”‚  â”œâ”€â”€ argocd/application.yaml                                                â”‚
â”‚  â”œâ”€â”€ k8s/base/                                                              â”‚
â”‚  â”œâ”€â”€ k8s/overlays/{env}/                                                   â”‚
â”‚  â””â”€â”€ terraform/                                                             â”‚
â”‚                                                                             â”‚
â”‚  .github/workflows/                        â† At REPO ROOT!                 â”‚
â”‚  â”œâ”€â”€ {app}-infra.yaml                      â† Infrastructure workflow       â”‚
â”‚  â””â”€â”€ {app}-deploy.yaml                     â† Application workflow          â”‚
â”‚                                                                             â”‚
â”‚  Step 4.3: Commit All Files                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚  git add .                                                                  â”‚
â”‚  git commit -m "feat({app}): Add deployment configuration for {env}"       â”‚
â”‚                                                                             â”‚
â”‚  Step 4.4: Push to Remote                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  git push -u origin {app}-{env}                                            â”‚
â”‚                                                                             â”‚
â”‚  âš ï¸  IMPORTANT: Pushing to Git triggers GitHub Actions automatically       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Phase 5: Execute (Infra â†’ Platform â†’ App)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXECUTION ORDER (Strict Sequence)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STEP 5a: INFRASTRUCTURE (GitHub Actions: {app}-infra.yaml)         â”‚   â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚   â”‚
â”‚  â”‚ Triggered by: Push to branch with terraform/ changes               â”‚   â”‚
â”‚  â”‚ Creates: VPC, EKS, ECR, IAM, ACM                                   â”‚   â”‚
â”‚  â”‚ Idempotent: Yes (terraform plan/apply)                             â”‚   â”‚
â”‚  â”‚ Duration: ~15-25 minutes                                           â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚ Monitor: gh run watch OR GitHub Actions tab                        â”‚   â”‚
â”‚  â”‚ Verify: aws eks list-clusters --region {region}                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼ (wait for completion)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STEP 5b: PLATFORM (Greenfield only)                                 â”‚   â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚   â”‚
â”‚  â”‚ Triggered by: Infra workflow completion (workflow_run)             â”‚   â”‚
â”‚  â”‚ Creates: ArgoCD, ExternalDNS, cert-manager                         â”‚   â”‚
â”‚  â”‚ Idempotent: Yes (kubectl apply with dry-run check)                 â”‚   â”‚
â”‚  â”‚ Duration: ~5-10 minutes                                            â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚ Verify: kubectl get pods -n argocd                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼ (wait for completion)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STEP 5c: APPLICATION (GitHub Actions: {app}-deploy.yaml)           â”‚   â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚   â”‚
â”‚  â”‚ Triggered by: Push to branch with app code changes                 â”‚   â”‚
â”‚  â”‚ Creates: Docker image, ECR push, K8s manifests, ArgoCD sync       â”‚   â”‚
â”‚  â”‚ Idempotent: Yes (ArgoCD handles drift)                             â”‚   â”‚
â”‚  â”‚ Duration: ~5-10 minutes                                            â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚ Monitor: gh run watch OR GitHub Actions tab                        â”‚   â”‚
â”‚  â”‚ Verify: kubectl get pods -n {app}-{env}                            â”‚   â”‚
â”‚  â”‚ Endpoint: kubectl get svc -n {app}-{env}                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flow Enforcement Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEVER DO THIS                          â”‚  ALWAYS DO THIS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âŒ Skip to creating files              â”‚  âœ… Collect ALL inputs first     â”‚
â”‚  âŒ Start infra without plan            â”‚  âœ… Show plan, get approval      â”‚
â”‚  âŒ Run terraform without prereq check  â”‚  âœ… Validate prerequisites       â”‚
â”‚  âŒ kubectl apply directly              â”‚  âœ… Commit to Git, let GHA run   â”‚
â”‚  âŒ Deploy app before infra ready       â”‚  âœ… Infra â†’ Platform â†’ App       â”‚
â”‚  âŒ Assume secrets are configured       â”‚  âœ… Verify GHA secrets exist     â”‚
â”‚  âŒ Use different regions for resources â”‚  âœ… Same region for EKS/ECR/ACM  â”‚
â”‚  âŒ Proceed on validation failure       â”‚  âœ… Stop and fix issues first    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ MANDATORY FIRST STEP: Initialize Deployment Context

**CRITICAL: Before ANY other action, you MUST collect these inputs and set up the workspace.**

### Step 0: Collect Required Inputs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REQUIRED INPUTS (Prompt user for these BEFORE proceeding)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. TENANT (required)                                                       â”‚
â”‚     â””â”€ Tenant/organization name (shared infrastructure scope)              â”‚
â”‚     â””â”€ Example: "awaaz", "acme", "opsera"                                  â”‚
â”‚     â””â”€ Used for: VPC, EKS clusters (shared across apps)                    â”‚
â”‚                                                                             â”‚
â”‚  2. APP_IDENTIFIER (required)                                               â”‚
â”‚     â””â”€ Name for this application (lowercase, hyphens allowed, no spaces)   â”‚
â”‚     â””â”€ Example: "myapp", "sales-dashboard", "api-v2"                       â”‚
â”‚     â””â”€ Used for: ECR repos, namespace, ArgoCD app (per-app)                â”‚
â”‚                                                                             â”‚
â”‚  3. AWS_REGION (required)                                                   â”‚
â”‚     â””â”€ Target AWS region for ALL resources (EKS, ECR, ACM)                  â”‚
â”‚     â””â”€ Options: us-west-1, us-west-2, us-east-1, eu-west-1, etc.           â”‚
â”‚     â””â”€ IMPORTANT: Use SAME region for everything to avoid cert issues      â”‚
â”‚                                                                             â”‚
â”‚  4. ENVIRONMENT (required)                                                  â”‚
â”‚     â””â”€ Target environment for deployment                                    â”‚
â”‚     â””â”€ Options: dev, staging, prod                                          â”‚
â”‚     â””â”€ Default: dev                                                         â”‚
â”‚                                                                             â”‚
â”‚  5. GITOPS_PATTERN (required)                                               â”‚
â”‚     â””â”€ GitOps architecture pattern to use                                   â”‚
â”‚     â””â”€ Options: greenfield, brownfield-embedded, brownfield-hub-spoke,     â”‚
â”‚                 brownfield-cluster                                          â”‚
â”‚     â””â”€ See "GitOps Architecture Patterns" section below for details        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Resource Scoping

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TENANT-LEVEL (shared infrastructure, created once per tenant)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚  â€¢ VPC: {tenant}-vpc                                                        â”‚
â”‚  â€¢ ArgoCD Cluster: {tenant}-argocd                                          â”‚
â”‚  â€¢ Workload Cluster: {tenant}-workload-{env}                                â”‚
â”‚  â€¢ IAM Roles: ExternalDNS, LB Controller                                    â”‚
â”‚                                                                             â”‚
â”‚  Managed by: Tenant-level Terraform (separate repo/folder)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APP-LEVEL (per application, created for each app)                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  â€¢ ECR Backend: {app}-backend                                               â”‚
â”‚  â€¢ ECR Frontend: {app}-frontend                                             â”‚
â”‚  â€¢ Namespace: {app}-{env}                                                   â”‚
â”‚  â€¢ ArgoCD App: {app}-argo-{env}                                             â”‚
â”‚  â€¢ Deployments, Services, Ingress                                           â”‚
â”‚                                                                             â”‚
â”‚  Managed by: App-level Terraform + K8s manifests (this deployment)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GitOps Architecture Patterns

**CRITICAL DISTINCTION: ArgoCD Cluster vs Target Cluster**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARGOCD CLUSTER (Management Plane)                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  â€¢ Runs ArgoCD server, repo-server, application-controller                  â”‚
â”‚  â€¢ Contains ArgoCD Application/ApplicationSet CRDs                          â”‚
â”‚  â€¢ Namespace: argocd (for ArgoCD components)                                â”‚
â”‚  â€¢ Namespace: argocd-apps (for Application manifests) - optional            â”‚
â”‚  â€¢ Does NOT run your application workloads                                  â”‚
â”‚                                                                             â”‚
â”‚  TARGET CLUSTER (Workload Plane)                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  â€¢ Where your actual application pods run                                   â”‚
â”‚  â€¢ Contains namespaces like: myapp-dev, myapp-prod                         â”‚
â”‚  â€¢ Runs your deployments, services, ingress                                 â”‚
â”‚  â€¢ Can be same cluster as ArgoCD (embedded) or different (hub-spoke)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Choose the pattern that matches your infrastructure:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PATTERN 1: GREENFIELD                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   ARGOCD CLUSTER    â”‚         â”‚   TARGET CLUSTER    â”‚                   â”‚
â”‚  â”‚      (NEW)          â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚      (NEW)          â”‚                   â”‚
â”‚  â”‚                     â”‚         â”‚                     â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚  â”‚  â”‚ argocd ns     â”‚  â”‚         â”‚  â”‚ myapp-dev ns  â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚ (ArgoCD pods) â”‚  â”‚         â”‚  â”‚ (your pods)   â”‚  â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚  â”‚  â”‚ Application   â”‚  â”‚         â”‚  â”‚ myapp-prod ns â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚ CRDs here     â”‚  â”‚         â”‚  â”‚ (your pods)   â”‚  â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                             â”‚
â”‚  â€¢ Create NEW ArgoCD cluster (management plane)                             â”‚
â”‚  â€¢ Create NEW target cluster (workload plane)                               â”‚
â”‚  â€¢ ArgoCD Applications live in ArgoCD cluster                               â”‚
â”‚  â€¢ Actual workloads deploy to target cluster                                â”‚
â”‚  â€¢ Use ApplicationSet for multi-env deployments                             â”‚
â”‚  â€¢ Best for: New projects, full isolation, enterprise setups               â”‚
â”‚                                                                             â”‚
â”‚  âš ï¸  PRIVATE SUBNET LIMITATION (Fix #45):                                   â”‚
â”‚  If clusters use private subnets, cross-cluster registration times out!    â”‚
â”‚  Solutions: VPC peering, public endpoints, or deploy to ArgoCD cluster.    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PATTERN 2: BROWNFIELD-EMBEDDED                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚      SINGLE CLUSTER (ArgoCD + Target)   â”‚                               â”‚
â”‚  â”‚                                         â”‚                               â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                               â”‚
â”‚  â”‚   â”‚  argocd ns      â”‚  (ArgoCD pods)    â”‚                               â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                               â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                               â”‚
â”‚  â”‚   â”‚  myapp-dev ns   â”‚  (your workloads) â”‚                               â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                               â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                               â”‚
â”‚  â”‚   â”‚  myapp-prod ns  â”‚  (your workloads) â”‚                               â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                               â”‚
â”‚  â”‚              (EXISTING)                 â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                                             â”‚
â”‚  â€¢ Use EXISTING cluster with ArgoCD already installed                       â”‚
â”‚  â€¢ ArgoCD deploys to same cluster (destination: in-cluster)                â”‚
â”‚  â€¢ Simplest setup, no cluster registration needed                           â”‚
â”‚  â€¢ Best for: Quick start, dev environments, single-cluster setups          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PATTERN 3: BROWNFIELD-HUB-SPOKE                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   ARGOCD CLUSTER    â”‚         â”‚   TARGET CLUSTER    â”‚                   â”‚
â”‚  â”‚    (EXISTING)       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    (EXISTING)       â”‚                   â”‚
â”‚  â”‚                     â”‚         â”‚                     â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚  â”‚  â”‚ argocd ns     â”‚  â”‚         â”‚  â”‚ myapp-dev ns  â”‚  â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”Œâ”€â”€â”€â–¶â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚  â”‚  â”‚ ApplicationSetâ”‚  â”‚    â”‚    â”‚  â”‚ myapp-prod ns â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚ (multi-env)   â”‚â”€â”€â”¼â”€â”€â”€â”€â”˜    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  â”‚         â”‚           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  OTHER CLUSTERS...  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                             â”‚
â”‚  â€¢ Use EXISTING ArgoCD cluster (hub)                                        â”‚
â”‚  â€¢ Use EXISTING target clusters (spokes)                                    â”‚
â”‚  â€¢ Register target clusters with ArgoCD                                     â”‚
â”‚  â€¢ Use ApplicationSet for multi-cluster deployments                         â”‚
â”‚  â€¢ Best for: Multi-cluster enterprise, shared ArgoCD                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PATTERN 4: BROWNFIELD-CLUSTER                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   ARGOCD CLUSTER    â”‚         â”‚   TARGET CLUSTER    â”‚                   â”‚
â”‚  â”‚      (NEW)          â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    (EXISTING)       â”‚                   â”‚
â”‚  â”‚                     â”‚         â”‚                     â”‚                   â”‚
â”‚  â”‚  Install ArgoCD     â”‚         â”‚  Your workloads     â”‚                   â”‚
â”‚  â”‚  here first         â”‚         â”‚  deploy here        â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                             â”‚
â”‚  â€¢ Create NEW ArgoCD installation                                           â”‚
â”‚  â€¢ Use EXISTING target cluster for workloads                                â”‚
â”‚  â€¢ Register existing cluster with new ArgoCD                                â”‚
â”‚  â€¢ Best for: Adding GitOps to existing infra, migration scenarios          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pattern Comparison Matrix

| Aspect | Greenfield | Brownfield-Embedded | Brownfield-Hub-Spoke | Brownfield-Cluster |
|--------|------------|---------------------|----------------------|-------------------|
| **ArgoCD Cluster** | Create NEW | Use EXISTING | Use EXISTING | Create NEW |
| **Target Cluster** | Create NEW | Same as ArgoCD | Use EXISTING | Use EXISTING |
| **Complexity** | High | Low | Medium | Medium |
| **Time to Deploy** | Hours | Minutes | Minutes | 30-60 min |
| **Best For** | New projects | Quick start | Enterprise | Migration |
| **Multi-cluster** | Yes | No | Yes | Optional |
| **Isolation** | Full | None | Full | Partial |
| **Cost** | Higher | Lowest | Medium | Medium |
| **ApplicationSet** | Recommended | Optional | Recommended | Optional |

---

## ApplicationSet for Multi-Environment Deployments

**Use ApplicationSet when deploying to multiple environments or clusters from a single definition.**

### When to Use ApplicationSet

| Scenario | Use Application | Use ApplicationSet |
|----------|----------------|-------------------|
| Single app, single env | âœ… | âŒ |
| Single app, multi env (dev/staging/prod) | âŒ | âœ… |
| Multi cluster deployments | âŒ | âœ… |
| Dynamic environment creation | âŒ | âœ… |

### ApplicationSet Template (Multi-Environment)

```yaml
# argocd/applicationset.yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: ${APP_IDENTIFIER}-environments
  namespace: argocd
  labels:
    tenant: ${TENANT}
    app-identifier: ${APP_IDENTIFIER}
spec:
  generators:
    - list:
        elements:
          - env: dev
            namespace: ${APP_IDENTIFIER}-dev
            cluster: https://kubernetes.default.svc  # or target cluster URL
            replicas: "1"
          - env: staging
            namespace: ${APP_IDENTIFIER}-staging
            cluster: https://kubernetes.default.svc
            replicas: "2"
          - env: prod
            namespace: ${APP_IDENTIFIER}-prod
            cluster: https://prod-cluster-api-url
            replicas: "3"
  template:
    metadata:
      name: '${APP_IDENTIFIER}-{{env}}'
      labels:
        tenant: ${TENANT}
        app-identifier: ${APP_IDENTIFIER}
        environment: '{{env}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/${ORG}/${REPO}.git
        targetRevision: ${APP_IDENTIFIER}-opsera
        path: ${APP_IDENTIFIER}-opsera/k8s/overlays/{{env}}
      destination:
        server: '{{cluster}}'
        namespace: '{{namespace}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
          - CreateNamespace=true
```

### ApplicationSet with Git Generator (Auto-detect environments)

```yaml
# Automatically creates apps for each overlay folder
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: ${APP_IDENTIFIER}-git-discovery
  namespace: argocd
spec:
  generators:
    - git:
        repoURL: https://github.com/${ORG}/${REPO}.git
        revision: ${APP_IDENTIFIER}-opsera
        directories:
          - path: ${APP_IDENTIFIER}-opsera/k8s/overlays/*
  template:
    metadata:
      name: '${APP_IDENTIFIER}-{{path.basename}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/${ORG}/${REPO}.git
        targetRevision: ${APP_IDENTIFIER}-opsera
        path: '{{path}}'
      destination:
        server: https://kubernetes.default.svc
        namespace: '${APP_IDENTIFIER}-{{path.basename}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
          - CreateNamespace=true
```

### Folder Structure for ApplicationSet

```
myapp-opsera/
â”œâ”€â”€ argocd/
â”‚   â””â”€â”€ applicationset.yaml      # Single ApplicationSet (replaces application.yaml)
â””â”€â”€ k8s/
    â”œâ”€â”€ base/                    # Shared base
    â””â”€â”€ overlays/
        â”œâ”€â”€ dev/                 # Auto-discovered by Git generator
        â”‚   â””â”€â”€ kustomization.yaml
        â”œâ”€â”€ staging/
        â”‚   â””â”€â”€ kustomization.yaml
        â””â”€â”€ prod/
            â””â”€â”€ kustomization.yaml
```

### Step 0.1: Prompt Template

Use this exact branded prompt to collect inputs:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                          â•‘
â•‘    â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘                         â•‘
â•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                         â•‘
â•‘     â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•                         â•‘
â•‘                                                                               â•‘
â•‘              â”â”â” AWS CONTAINER GITOPS DEPLOYMENT â”â”â”                         â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                        Powered by Opsera DevOps Platform

Before we begin, I need five pieces of information:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. TENANT                                                                   â”‚
â”‚     What is the tenant/organization name?                                    â”‚
â”‚     â–º This is the shared infrastructure scope (VPC, clusters)               â”‚
â”‚     â–º Example: "awaaz", "acme", "opsera"                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. APP IDENTIFIER                                                           â”‚
â”‚     What name would you like for this application?                           â”‚
â”‚     â–º Must be lowercase, hyphens allowed, no spaces                         â”‚
â”‚     â–º Example: "myapp", "sales-dashboard", "api-v2"                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. AWS REGION                                                               â”‚
â”‚     Which AWS region should we deploy to?                                    â”‚
â”‚     â–º Common options: us-west-1, us-west-2, us-east-1, eu-west-1           â”‚
â”‚     â–º Use SAME region for EKS, ECR, and ACM to avoid issues                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. ENVIRONMENT                                                              â”‚
â”‚     Which environment is this for?                                           â”‚
â”‚     â–º Options: dev, staging, prod                                           â”‚
â”‚     â–º Default: dev                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. GITOPS PATTERN                                                           â”‚
â”‚     Which architecture pattern should we use?                                â”‚
â”‚                                                                              â”‚
â”‚     â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®â”‚
â”‚     â”‚  A) GREENFIELD                                                       â”‚â”‚
â”‚     â”‚     Create NEW tenant infrastructure (VPC, clusters)                 â”‚â”‚
â”‚     â”‚     Best for: New tenants, full control                              â”‚â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚     â”‚  B) BROWNFIELD-EMBEDDED â˜… Recommended for quick start               â”‚â”‚
â”‚     â”‚     Use EXISTING tenant cluster with ArgoCD already installed        â”‚â”‚
â”‚     â”‚     Best for: Adding apps to existing tenant infrastructure          â”‚â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚     â”‚  C) BROWNFIELD-HUB-SPOKE                                             â”‚â”‚
â”‚     â”‚     Use EXISTING ArgoCD hub + EXISTING workload cluster              â”‚â”‚
â”‚     â”‚     Best for: Enterprise, multi-cluster setups                       â”‚â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚     â”‚  D) BROWNFIELD-CLUSTER                                               â”‚â”‚
â”‚     â”‚     Use EXISTING cluster + create NEW ArgoCD                         â”‚â”‚
â”‚     â”‚     Best for: Adding GitOps to existing infrastructure               â”‚â”‚
â”‚     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Your app will be available at: https://<app>-<env>.agents.opsera-labs.com

Branch/Folder naming: {app}-opsera (e.g., myapp-opsera)
ArgoCD manifests go in: {app}-opsera/argocd/ (FLAT, no gitops/ prefix)

Please provide all five values.
```

### Step 0.2: Create Branch and Folder Structure

Once inputs are collected, **IMMEDIATELY** execute:

```bash
# Set variables from user input
TENANT="<user-provided-tenant>"                            # e.g., awaaz
APP_IDENTIFIER="<user-provided-identifier>"                # e.g., myapp
AWS_REGION="<user-provided-region>"                        # e.g., us-west-1
ENVIRONMENT="<user-provided-env>"                          # dev, staging, or prod
GITOPS_PATTERN="<user-provided-pattern>"                   # greenfield, brownfield-embedded, brownfield-hub-spoke, brownfield-cluster

# Derive naming components
DEPLOYMENT_NAME="${APP_IDENTIFIER}-opsera"                 # e.g., myapp-opsera (branch & folder)
ARGO_APP_NAME="${APP_IDENTIFIER}-argo-${ENVIRONMENT}"     # e.g., myapp-argo-dev
NAMESPACE="${APP_IDENTIFIER}-${ENVIRONMENT}"               # e.g., myapp-dev

# Create and switch to new branch
git checkout -b ${DEPLOYMENT_NAME}

# Create deployment folder structure (deployment configs only, no app code)
# NOTE: .github/workflows is created at REPO ROOT, not inside deployment folder!
mkdir -p ${DEPLOYMENT_NAME}/{argocd,terraform,k8s/base,k8s/overlays/${ENVIRONMENT}}

# CRITICAL: Create GitHub Actions workflow at REPO ROOT (not in deployment folder!)
# GitHub Actions ONLY reads from repo root's .github/workflows/ folder
mkdir -p .github/workflows

# Create deployment context file (tracks all config)
cat > ${DEPLOYMENT_NAME}/DEPLOYMENT-CONTEXT.md << EOF
# Deployment Context

## Identifiers
- **Tenant**: ${TENANT}
- **App Identifier**: ${APP_IDENTIFIER}
- **Environment**: ${ENVIRONMENT}
- **Branch/Folder**: ${DEPLOYMENT_NAME} (${APP_IDENTIFIER}-opsera)
- **ArgoCD Folder**: ${DEPLOYMENT_NAME}/argocd/

## GitOps Architecture
- **Pattern**: ${GITOPS_PATTERN}
- **ArgoCD**: $(case ${GITOPS_PATTERN} in greenfield|brownfield-cluster) echo "CREATE NEW";; *) echo "USE EXISTING";; esac)
- **Workload Cluster**: $(case ${GITOPS_PATTERN} in greenfield) echo "CREATE NEW";; *) echo "USE EXISTING";; esac)

## Resource Names
- **ArgoCD App**: ${ARGO_APP_NAME}
- **Namespace**: ${NAMESPACE}
- **ECR Backend**: ${APP_IDENTIFIER}-backend
- **ECR Frontend**: ${APP_IDENTIFIER}-frontend

## AWS Configuration
- **Region**: ${AWS_REGION} (used for EKS, ECR, and ACM - keep consistent!)
- **Account ID**: $(aws sts get-caller-identity --query Account --output text)
- **EKS Cluster**: ${TENANT}-argocd / ${TENANT}-workload-${ENVIRONMENT}

## Resource Tags (apply to ALL cloud resources)
\`\`\`
app-identifier: ${APP_IDENTIFIER}
environment: ${ENVIRONMENT}
deployment-name: ${DEPLOYMENT_NAME}
gitops-pattern: ${GITOPS_PATTERN}
managed-by: opsera-gitops
created-by: claude-code
\`\`\`

## Endpoints (after deployment)
- **URL**: https://${DEPLOYMENT_NAME}.agents.opsera-labs.com

## Pattern-Specific Checklist

### ${GITOPS_PATTERN} Pattern Steps:
$(case ${GITOPS_PATTERN} in
  greenfield)
    echo "- [ ] Create ArgoCD management cluster (Terraform)"
    echo "- [ ] Create workload cluster (Terraform)"
    echo "- [ ] Install ArgoCD on management cluster"
    echo "- [ ] Register workload cluster with ArgoCD"
    echo "- [ ] Install ExternalDNS on workload cluster"
    ;;
  brownfield-embedded)
    echo "- [ ] Verify existing cluster access"
    echo "- [ ] Verify ArgoCD is installed and accessible"
    echo "- [ ] Create ArgoCD repo secret"
    ;;
  brownfield-hub-spoke)
    echo "- [ ] Verify ArgoCD hub cluster access"
    echo "- [ ] Verify workload cluster access"
    echo "- [ ] Register workload cluster with ArgoCD (if not already)"
    echo "- [ ] Create ArgoCD repo secret"
    ;;
  brownfield-cluster)
    echo "- [ ] Verify existing cluster access"
    echo "- [ ] Install ArgoCD on cluster"
    echo "- [ ] Configure ArgoCD"
    echo "- [ ] Install ExternalDNS"
    echo "- [ ] Create ArgoCD repo secret"
    ;;
esac)

## Common Steps (All Patterns):
- [ ] Branch created
- [ ] Folder structure created
- [ ] Initial commit pushed
- [ ] ECR repositories created (tagged)
- [ ] Namespace created (labeled)
- [ ] ArgoCD application created: ${ARGO_APP_NAME}
- [ ] Deployment verified

---
Created: $(date -u +"%Y-%m-%dT%H:%M:%SZ")
EOF

# Initial commit - COMMIT EARLY!
git add ${DEPLOYMENT_NAME}/
git commit -m "Initialize ${DEPLOYMENT_NAME} deployment structure

- App Identifier: ${APP_IDENTIFIER}
- Environment: ${ENVIRONMENT}
- GitOps Pattern: ${GITOPS_PATTERN}
- Branch: ${DEPLOYMENT_NAME}
- Folder: ${DEPLOYMENT_NAME}/
- AWS Region: ${AWS_REGION}
- ArgoCD App: ${ARGO_APP_NAME}

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"

# Push branch to remote
git push -u origin ${DEPLOYMENT_NAME}

echo "âœ… Deployment context initialized!"
echo "   Branch: ${DEPLOYMENT_NAME}"
echo "   Folder: ${DEPLOYMENT_NAME}/"
echo "   GitOps Pattern: ${GITOPS_PATTERN}"
echo "   ArgoCD App: ${ARGO_APP_NAME}"
echo "   Namespace: ${NAMESPACE}"
echo "   Pushed to remote"
```

### Step 0.3: Verification Checklist

Before proceeding to Layer 1-4, confirm:

- [ ] `APP_IDENTIFIER` collected from user
- [ ] `AWS_REGION` collected from user
- [ ] `ENVIRONMENT` collected from user (dev/staging/prod)
- [ ] `GITOPS_PATTERN` collected from user (greenfield/brownfield-embedded/brownfield-hub-spoke/brownfield-cluster)
- [ ] Branch `{identifier}-{env}` created and checked out
- [ ] Folder `{identifier}-{env}/` created with structure
- [ ] `DEPLOYMENT-CONTEXT.md` created in folder (includes pattern-specific checklist)
- [ ] Initial commit pushed to remote

**DO NOT proceed until all items are checked.**

---

## Pattern-Specific Setup Guides

Based on the selected `GITOPS_PATTERN`, follow the appropriate guide:

### GREENFIELD Pattern Setup

**Prerequisites**: AWS account access, Terraform installed

**Creates**: NEW ArgoCD cluster (management) + NEW Target cluster (workloads)

```bash
# ============================================================================
# STEP 1: Create VPC (shared by both clusters)
# ============================================================================
cd ${DEPLOYMENT_NAME}/terraform

cat > vpc.tf << 'TFEOF'
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  name = "${var.tenant}-vpc"
  cidr = "10.0.0.0/16"

  azs             = ["${var.aws_region}a", "${var.aws_region}b"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24"]

  enable_nat_gateway   = true
  single_nat_gateway   = true
  enable_dns_hostnames = true

  tags = {
    tenant     = var.tenant
    managed-by = "opsera-gitops"
  }
}
TFEOF

# ============================================================================
# STEP 2: Create ArgoCD Cluster (Management Plane)
# ============================================================================
cat > argocd-cluster.tf << 'TFEOF'
module "argocd_cluster" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"

  cluster_name    = "${var.tenant}-argocd"
  cluster_version = "1.29"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  # ArgoCD cluster - smaller, just runs ArgoCD
  eks_managed_node_groups = {
    argocd = {
      instance_types = ["t3.medium"]
      min_size       = 2
      max_size       = 3
      desired_size   = 2
    }
  }

  # Enable IRSA for ExternalDNS, etc.
  enable_irsa = true

  tags = {
    tenant     = var.tenant
    role       = "argocd-management"
    managed-by = "opsera-gitops"
  }
}
TFEOF

# ============================================================================
# STEP 3: Create Target Cluster (Workload Plane)
# ============================================================================
cat > target-cluster.tf << 'TFEOF'
module "target_cluster" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"

  cluster_name    = "${var.tenant}-workload-${var.environment}"
  cluster_version = "1.29"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  # Target cluster - larger, runs actual workloads
  eks_managed_node_groups = {
    workload = {
      instance_types = ["t3.large"]  # 35 pods per node
      min_size       = 2
      max_size       = 10
      desired_size   = 3
    }
  }

  # Enable IRSA for ExternalDNS, AWS LB Controller
  enable_irsa = true

  tags = {
    tenant      = var.tenant
    environment = var.environment
    role        = "workload"
    managed-by  = "opsera-gitops"
  }
}
TFEOF

# ============================================================================
# STEP 4: Apply Terraform
# ============================================================================
terraform init && terraform apply

# ============================================================================
# STEP 5: Install ArgoCD on ArgoCD Cluster
# ============================================================================
# Connect to ArgoCD cluster
aws eks update-kubeconfig --name ${TENANT}-argocd --region ${AWS_REGION} --alias argocd-cluster

kubectl --context argocd-cluster create namespace argocd
kubectl --context argocd-cluster apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Wait for ArgoCD to be ready
kubectl --context argocd-cluster wait --for=condition=available deployment/argocd-server -n argocd --timeout=300s

# Get ArgoCD admin password
ARGOCD_PASSWORD=$(kubectl --context argocd-cluster -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
echo "ArgoCD Password: $ARGOCD_PASSWORD"

# ============================================================================
# STEP 6: Install ExternalDNS on Target Cluster
# ============================================================================
# Connect to Target cluster
aws eks update-kubeconfig --name ${TENANT}-workload-${ENVIRONMENT} --region ${AWS_REGION} --alias target-cluster

kubectl --context target-cluster apply -f external-dns.yaml

# ============================================================================
# STEP 7: Register Target Cluster with ArgoCD
# ============================================================================
# Port-forward ArgoCD (in background)
kubectl --context argocd-cluster port-forward svc/argocd-server -n argocd 8080:443 &

# Login to ArgoCD CLI
argocd login localhost:8080 --username admin --password $ARGOCD_PASSWORD --insecure

# Add target cluster to ArgoCD
argocd cluster add target-cluster --name ${TENANT}-workload-${ENVIRONMENT}

# Verify cluster is registered
argocd cluster list

# ============================================================================
# STEP 8: Create ApplicationSet for Multi-Environment
# ============================================================================
# Applications are created in ArgoCD cluster, workloads deploy to target cluster
kubectl --context argocd-cluster apply -f ${DEPLOYMENT_NAME}/argocd/applicationset.yaml
```

**Key Points for Greenfield**:
- ArgoCD cluster runs ArgoCD components only (namespace: `argocd`)
- Target cluster runs your workloads (namespaces: `myapp-dev`, `myapp-prod`)
- ArgoCD Application CRDs live in ArgoCD cluster
- Use `argocd cluster add` to register target cluster
- Use ApplicationSet for multi-environment deployments

---

### BROWNFIELD-EMBEDDED Pattern Setup

**Prerequisites**: Existing cluster with ArgoCD installed

```bash
# Step 1: Verify cluster access
kubectl cluster-info
kubectl get nodes

# Step 2: Verify ArgoCD is running
kubectl get pods -n argocd
kubectl get svc -n argocd

# Step 3: Get ArgoCD credentials
ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
echo "ArgoCD Password: $ARGOCD_PASSWORD"

# Step 4: Port-forward to access ArgoCD UI (optional)
kubectl port-forward svc/argocd-server -n argocd 8080:443 &

# Step 5: Create repo secret for your Git repository
kubectl create secret generic repo-${DEPLOYMENT_NAME} \
  --namespace argocd \
  --from-literal=url=git@github.com:<org>/<repo>.git \
  --from-literal=sshPrivateKey="$(cat ~/.ssh/id_rsa)" \
  --from-literal=type=git \
  --dry-run=client -o yaml | kubectl apply -f -

kubectl label secret repo-${DEPLOYMENT_NAME} -n argocd argocd.argoproj.io/secret-type=repository --overwrite
```

---

### BROWNFIELD-HUB-SPOKE Pattern Setup

**Prerequisites**: Existing ArgoCD hub cluster, existing workload cluster

```bash
# Step 1: Connect to ArgoCD hub cluster
aws eks update-kubeconfig --name <argocd-hub-cluster> --region ${AWS_REGION} --alias argocd-hub

# Step 2: Verify ArgoCD is running on hub
kubectl --context argocd-hub get pods -n argocd

# Step 3: Connect to workload cluster
aws eks update-kubeconfig --name <workload-cluster> --region ${AWS_REGION} --alias workload

# Step 4: Check if workload cluster is registered with ArgoCD
argocd cluster list

# Step 5: If not registered, add workload cluster to ArgoCD
argocd cluster add workload --name ${APP_IDENTIFIER}-workload-${ENVIRONMENT}

# Step 6: Create repo secret on ArgoCD hub
kubectl --context argocd-hub create secret generic repo-${DEPLOYMENT_NAME} \
  --namespace argocd \
  --from-literal=url=git@github.com:<org>/<repo>.git \
  --from-literal=sshPrivateKey="$(cat ~/.ssh/id_rsa)" \
  --from-literal=type=git \
  --dry-run=client -o yaml | kubectl apply -f -

kubectl --context argocd-hub label secret repo-${DEPLOYMENT_NAME} -n argocd argocd.argoproj.io/secret-type=repository --overwrite

# Step 7: ArgoCD Application should target workload cluster
# In your ArgoCD Application manifest, set:
#   destination:
#     server: <workload-cluster-api-server-url>
#     namespace: ${NAMESPACE}
```

---

### BROWNFIELD-CLUSTER Pattern Setup

**Prerequisites**: Existing Kubernetes cluster

```bash
# Step 1: Verify existing cluster access
kubectl cluster-info
kubectl get nodes

# Step 2: Install ArgoCD on existing cluster
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Wait for ArgoCD to be ready
kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd

# Step 3: Get ArgoCD initial password
ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
echo "ArgoCD Password: $ARGOCD_PASSWORD"

# Step 4: Install ExternalDNS (for automatic DNS)
kubectl apply -f - << 'EDNSEOF'
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: external-dns
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: external-dns
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.k8s.io/external-dns/external-dns:v0.14.0
        args:
        - --source=service
        - --domain-filter=agents.opsera-labs.com
        - --provider=aws
        - --aws-zone-type=public
        - --registry=txt
        - --txt-owner-id=external-dns
EDNSEOF

# Step 5: Create repo secret
kubectl create secret generic repo-${DEPLOYMENT_NAME} \
  --namespace argocd \
  --from-literal=url=git@github.com:<org>/<repo>.git \
  --from-literal=sshPrivateKey="$(cat ~/.ssh/id_rsa)" \
  --from-literal=type=git \
  --dry-run=client -o yaml | kubectl apply -f -

kubectl label secret repo-${DEPLOYMENT_NAME} -n argocd argocd.argoproj.io/secret-type=repository --overwrite
```

---

## Naming Convention

Resources follow a consistent pattern using `{app}` and `{env}`:

| Resource | Naming Pattern | Example (app=myapp, env=dev) |
|----------|---------------|------------------------------|
| **Git Branch** | `{app}-opsera` | `myapp-opsera` |
| **Folder** | `{app}-opsera/` | `myapp-opsera/` |
| **ArgoCD Folder** | `{app}-opsera/argocd/` | `myapp-opsera/argocd/` |
| Namespace | `{app}-{env}` | `myapp-dev` |
| ArgoCD App | `{app}-argo-{env}` | `myapp-argo-dev` |
| ECR Backend | `{app}-backend` | `myapp-backend` |
| ECR Frontend | `{app}-frontend` | `myapp-frontend` |
| DNS | `{app}-{env}.agents.opsera-labs.com` | `myapp-dev.agents.opsera-labs.com` |
| ArgoCD Secret | `repo-{app}-opsera` | `repo-myapp-opsera` |
| Backend Deployment | `{app}-backend` | `myapp-backend` |
| Frontend Deployment | `{app}-frontend` | `myapp-frontend` |
| Backend Service | `{app}-backend` | `myapp-backend` |
| Frontend Service | `{app}-frontend` | `myapp-frontend` |

**Key Insight**: The `{app}-opsera` folder contains ONLY deployment configs (argocd/, terraform/, k8s/, workflows). Application source code lives separately (either in repo root or a different repo).

---

## Resource Tagging Standard

**CRITICAL: ALL cloud resources MUST be tagged with these labels.**

### AWS Resource Tags (ECR, IAM, etc.)

```bash
# Apply these tags to ALL AWS resources
aws ecr create-repository --repository-name ${APP_IDENTIFIER}-backend \
  --tags Key=app-identifier,Value=${APP_IDENTIFIER} \
         Key=environment,Value=${ENVIRONMENT} \
         Key=deployment-name,Value=${DEPLOYMENT_NAME} \
         Key=managed-by,Value=opsera-gitops \
         Key=created-by,Value=claude-code
```

### Kubernetes Labels (Namespace, Deployments, Services)

```yaml
metadata:
  labels:
    app-identifier: "${APP_IDENTIFIER}"
    environment: "${ENVIRONMENT}"
    deployment-name: "${DEPLOYMENT_NAME}"
    managed-by: "opsera-gitops"
    created-by: "claude-code"
```

### Namespace with Labels

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ${APP_IDENTIFIER}-${ENVIRONMENT}
  labels:
    app-identifier: "${APP_IDENTIFIER}"
    environment: "${ENVIRONMENT}"
    deployment-name: "${DEPLOYMENT_NAME}"
    managed-by: "opsera-gitops"
    created-by: "claude-code"
```

### ArgoCD Application with Labels

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ${APP_IDENTIFIER}-argo-${ENVIRONMENT}   # e.g., myapp-argo-dev
  namespace: argocd
  labels:
    app-identifier: "${APP_IDENTIFIER}"
    environment: "${ENVIRONMENT}"
    deployment-name: "${DEPLOYMENT_NAME}"
    managed-by: "opsera-gitops"
    created-by: "claude-code"
spec:
  project: default
  source:
    repoURL: <git-repo-url>
    targetRevision: ${DEPLOYMENT_NAME}           # Branch name
    path: ${DEPLOYMENT_NAME}/k8s/overlays/${ENVIRONMENT}
  destination:
    server: https://kubernetes.default.svc
    namespace: ${APP_IDENTIFIER}-${ENVIRONMENT}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

### Tag Reference Table

| Tag Key | Value | Purpose |
|---------|-------|---------|
| `app-identifier` | User-provided identifier | Links all resources for an app |
| `environment` | dev/staging/prod | Identifies target environment |
| `deployment-name` | `{identifier}-{env}` | Full deployment reference |
| `managed-by` | `opsera-gitops` | Indicates GitOps management |
| `created-by` | `claude-code` | Indicates automation source |

---

## Pre-Deployment Verification Checklist

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PRE-DEPLOYMENT CHECKLIST                                 Powered by Opsera  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  Run through this checklist BEFORE starting deployment:                       â•‘
â•‘                                                                               â•‘
â•‘  INFRASTRUCTURE                                                               â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â–¡ AWS credentials configured (aws sts get-caller-identity)                   â•‘
â•‘  â–¡ Correct AWS account selected (check account ID)                            â•‘
â•‘  â–¡ kubectl connected to correct cluster (kubectl cluster-info)                â•‘
â•‘  â–¡ ArgoCD accessible (kubectl get pods -n argocd)                            â•‘
â•‘  â–¡ ExternalDNS running (kubectl get pods -l app=external-dns -n kube-system)  â•‘
â•‘                                                                               â•‘
â•‘  CERTIFICATES & DNS                                                           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â–¡ ACM certificate ISSUED (not PENDING_VALIDATION)                            â•‘
â•‘  â–¡ Certificate ARN verified: aws acm list-certificates --region <region>      â•‘
â•‘  â–¡ Route53 hosted zone exists                                                 â•‘
â•‘  â–¡ Nameservers propagated (dig domain.com NS @8.8.8.8)                       â•‘
â•‘                                                                               â•‘
â•‘  CODE & CONFIGURATION                                                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â–¡ Backend has /health endpoint                                               â•‘
â•‘  â–¡ Backend CORS includes production URL                                       â•‘
â•‘  â–¡ Frontend nginx.conf has correct backend service name                       â•‘
â•‘  â–¡ Dockerfiles use correct base images and ports                              â•‘
â•‘  â–¡ package-lock.json committed (for npm ci)                                   â•‘
â•‘                                                                               â•‘
â•‘  GIT & ARGOCD                                                                 â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â–¡ Branch created and pushed to remote                                        â•‘
â•‘  â–¡ ArgoCD repo secret exists with HTTPS URL (not SSH)                         â•‘
â•‘  â–¡ ArgoCD Application uses HTTPS repoURL                                      â•‘
â•‘  â–¡ Kustomization.yaml image tags are correct                                  â•‘
â•‘  â–¡ terraform/.gitignore excludes .terraform/ and *.tfstate                    â•‘
â•‘                                                                               â•‘
â•‘  KUBERNETES                                                                   â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â–¡ Namespace doesn't already exist (or cleanup completed)                     â•‘
â•‘  â–¡ Node capacity available (kubectl get nodes -o wide)                        â•‘
â•‘  â–¡ No stuck Terminating pods from previous deployments                        â•‘
â•‘  â–¡ Service annotations have correct ACM cert ARN                              â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Quick Verification Commands

```bash
# 1. Check AWS identity
aws sts get-caller-identity

# 2. Check cluster connection
kubectl cluster-info
kubectl get nodes

# 3. Check ArgoCD
kubectl get pods -n argocd | head -5

# 4. Check ExternalDNS
kubectl get pods -l app=external-dns -n kube-system

# 5. Check ACM certificate (MUST be ISSUED)
aws acm list-certificates --region us-west-1 \
  --query 'CertificateSummaryList[?Status==`ISSUED`].[DomainName,CertificateArn]' \
  --output table

# 6. Check for existing namespace (should not exist for new deployment)
kubectl get namespace | grep <your-namespace>

# 7. Check node pod capacity
kubectl describe nodes | grep -A 5 "Allocated resources"

# 8. Check for stuck pods
kubectl get pods -A | grep -E "Terminating|Pending|Error"
```

---

## âš ï¸ AWS CAPACITY & AVAILABILITY PRE-FLIGHT CHECKS

**CRITICAL: Run these checks BEFORE creating any AWS resources to avoid partial deployments and failures.**

### Why This Matters
Deployments often fail mid-way due to:
- Region doesn't support required services
- VPC/NAT Gateway limits exceeded
- Node group capacity unavailable
- EIP limits reached
- EKS version not available in region

Running pre-flight checks prevents painful rollbacks and manual cleanup.

### Pre-Flight Check Script

```bash
#!/bin/bash
# AWS Pre-flight Checks - Run BEFORE any infrastructure creation
# Usage: ./preflight-checks.sh <REGION> <TENANT> <APP_IDENTIFIER>
# All parameters required - no defaults to avoid region mismatch issues

set -e
AWS_REGION="${1:?'AWS_REGION required (e.g., us-west-2)'}"
TENANT="${2:?'TENANT required (e.g., mycompany)'}"
APP_IDENTIFIER="${3:?'APP_IDENTIFIER required (e.g., myapp)'}"
CLUSTER_NAME="${TENANT}-workload"

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  AWS CAPACITY & AVAILABILITY PRE-FLIGHT CHECKS                                â•‘"
echo "â•‘  Region: $AWS_REGION | Tenant: $TENANT                                        â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

CHECKS_PASSED=0
CHECKS_FAILED=0

check_pass() { echo "  âœ… $1"; ((CHECKS_PASSED++)); }
check_fail() { echo "  âŒ $1"; ((CHECKS_FAILED++)); }
check_warn() { echo "  âš ï¸  $1"; }

# ============================================================================
# 1. VPC CAPACITY CHECK
# ============================================================================
echo ""
echo "ğŸ“¦ VPC CAPACITY"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

# Check existing VPCs in region
VPC_COUNT=$(aws ec2 describe-vpcs --region $AWS_REGION --query 'length(Vpcs)' --output text)
VPC_LIMIT=5  # Default limit is 5 VPCs per region

if [ "$VPC_COUNT" -lt "$VPC_LIMIT" ]; then
    check_pass "VPC capacity available: $VPC_COUNT/$VPC_LIMIT used"
else
    check_fail "VPC limit reached: $VPC_COUNT/$VPC_LIMIT - Request limit increase"
fi

# Check for existing tenant VPC (for reuse)
EXISTING_VPC=$(aws ec2 describe-vpcs --region $AWS_REGION \
    --filters "Name=tag:tenant,Values=$TENANT" \
    --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
if [ "$EXISTING_VPC" != "None" ] && [ -n "$EXISTING_VPC" ]; then
    check_warn "Existing tenant VPC found: $EXISTING_VPC (will reuse)"
fi

# ============================================================================
# 2. NAT GATEWAY & EIP LIMITS
# ============================================================================
echo ""
echo "ğŸŒ NAT GATEWAY & ELASTIC IP CAPACITY"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

# Check NAT Gateway count
NAT_COUNT=$(aws ec2 describe-nat-gateways --region $AWS_REGION \
    --filter "Name=state,Values=available,pending" \
    --query 'length(NatGateways)' --output text)
NAT_LIMIT=5  # Default limit

if [ "$NAT_COUNT" -lt "$NAT_LIMIT" ]; then
    check_pass "NAT Gateway capacity: $NAT_COUNT/$NAT_LIMIT used"
else
    check_fail "NAT Gateway limit reached: $NAT_COUNT/$NAT_LIMIT"
fi

# Check Elastic IP allocation
EIP_COUNT=$(aws ec2 describe-addresses --region $AWS_REGION --query 'length(Addresses)' --output text)
EIP_LIMIT=5  # Default limit

if [ "$EIP_COUNT" -lt "$EIP_LIMIT" ]; then
    check_pass "Elastic IP capacity: $EIP_COUNT/$EIP_LIMIT used"
else
    check_fail "Elastic IP limit reached: $EIP_COUNT/$EIP_LIMIT"
fi

# ============================================================================
# 3. EKS CLUSTER AVAILABILITY
# ============================================================================
echo ""
echo "â˜¸ï¸  EKS CLUSTER CHECKS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

# Check if EKS is available in region
EKS_AVAILABLE=$(aws eks list-clusters --region $AWS_REGION --query 'length(clusters)' --output text 2>/dev/null && echo "yes" || echo "no")
if [ "$EKS_AVAILABLE" = "no" ]; then
    check_fail "EKS not available in region $AWS_REGION"
else
    check_pass "EKS available in $AWS_REGION"
fi

# Check for existing cluster (for brownfield patterns)
EXISTING_CLUSTER=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION \
    --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")
if [ "$EXISTING_CLUSTER" = "ACTIVE" ]; then
    check_pass "Existing cluster found: $CLUSTER_NAME (ACTIVE) - can reuse"
elif [ "$EXISTING_CLUSTER" = "CREATING" ]; then
    check_warn "Cluster $CLUSTER_NAME is still CREATING - wait before proceeding"
elif [ "$EXISTING_CLUSTER" = "NOT_FOUND" ]; then
    check_pass "No existing cluster $CLUSTER_NAME - will create new"
else
    check_fail "Cluster $CLUSTER_NAME in unexpected state: $EXISTING_CLUSTER"
fi

# Check EKS version availability
EKS_VERSIONS=$(aws eks describe-addon-versions --region $AWS_REGION \
    --query 'addons[0].addonVersions[0].compatibilities[*].clusterVersion' \
    --output text 2>/dev/null | head -1)
if [ -n "$EKS_VERSIONS" ]; then
    check_pass "EKS versions available: $EKS_VERSIONS"
fi

# ============================================================================
# 4. NODE GROUP CAPACITY (Instance Availability)
# ============================================================================
echo ""
echo "ğŸ–¥ï¸  NODE CAPACITY & INSTANCE AVAILABILITY"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

# Check if t3.large is available (common choice for workloads)
for INSTANCE_TYPE in t3.medium t3.large m5.large; do
    AZ_AVAILABLE=$(aws ec2 describe-instance-type-offerings --region $AWS_REGION \
        --location-type availability-zone \
        --filters "Name=instance-type,Values=$INSTANCE_TYPE" \
        --query 'length(InstanceTypeOfferings)' --output text)
    if [ "$AZ_AVAILABLE" -gt 0 ]; then
        check_pass "$INSTANCE_TYPE available in $AZ_AVAILABLE AZs"
    else
        check_warn "$INSTANCE_TYPE NOT available in $AWS_REGION"
    fi
done

# ============================================================================
# 5. ECR REPOSITORY CHECK
# ============================================================================
echo ""
echo "ğŸ“¦ ECR REPOSITORY CHECK"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

# Use same region as EKS for ECR (avoid region mismatch - Fix #44)
for REPO_SUFFIX in backend frontend; do
    REPO_NAME="${APP_IDENTIFIER}-${REPO_SUFFIX}"
    REPO_EXISTS=$(aws ecr describe-repositories --repository-names $REPO_NAME \
        --region $AWS_REGION --query 'repositories[0].repositoryUri' --output text 2>/dev/null || echo "NOT_FOUND")
    if [ "$REPO_EXISTS" != "NOT_FOUND" ]; then
        check_pass "ECR repo exists: $REPO_NAME (will reuse)"
    else
        check_pass "ECR repo $REPO_NAME will be created"
    fi
done

# ============================================================================
# 6. IAM ROLE CHECK (for idempotency)
# ============================================================================
echo ""
echo "ğŸ” IAM ROLE CHECK"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

for ROLE_SUFFIX in "external-dns" "eks-node" "eks-cluster"; do
    ROLE_NAME="${TENANT}-${ROLE_SUFFIX}"
    ROLE_EXISTS=$(aws iam get-role --role-name $ROLE_NAME \
        --query 'Role.Arn' --output text 2>/dev/null || echo "NOT_FOUND")
    if [ "$ROLE_EXISTS" != "NOT_FOUND" ]; then
        check_warn "IAM role exists: $ROLE_NAME (Terraform will import or reuse)"
    fi
done

# ============================================================================
# 7. ROUTE53 HOSTED ZONE CHECK
# ============================================================================
echo ""
echo "ğŸŒ ROUTE53 HOSTED ZONE CHECK"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

DOMAIN="opsera-labs.com"
ZONE_EXISTS=$(aws route53 list-hosted-zones-by-name --dns-name $DOMAIN \
    --query "HostedZones[?Name=='$DOMAIN.'].Id" --output text 2>/dev/null || echo "")
if [ -n "$ZONE_EXISTS" ]; then
    check_pass "Route53 zone exists for $DOMAIN"
else
    check_warn "Route53 zone for $DOMAIN not found - may need to create"
fi

# ============================================================================
# 8. SERVICE QUOTAS SUMMARY
# ============================================================================
echo ""
echo "ğŸ“Š SERVICE QUOTAS (Request increases if needed)"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo "  Check quotas at: https://console.aws.amazon.com/servicequotas/"
echo "  - VPCs per region: $VPC_COUNT/$VPC_LIMIT"
echo "  - NAT Gateways per AZ: $NAT_COUNT/$NAT_LIMIT"
echo "  - Elastic IPs: $EIP_COUNT/$EIP_LIMIT"

# ============================================================================
# SUMMARY
# ============================================================================
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
if [ $CHECKS_FAILED -eq 0 ]; then
    echo "â•‘  âœ… ALL CHECKS PASSED: $CHECKS_PASSED checks passed, $CHECKS_FAILED failed                           â•‘"
    echo "â•‘  Safe to proceed with deployment                                              â•‘"
else
    echo "â•‘  âŒ CHECKS FAILED: $CHECKS_PASSED passed, $CHECKS_FAILED failed                                      â•‘"
    echo "â•‘  Fix issues above before proceeding                                           â•‘"
fi
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Exit with error if any checks failed
[ $CHECKS_FAILED -eq 0 ] || exit 1
```

### Quick Pre-Flight Commands

```bash
# Run all pre-flight checks
./preflight-checks.sh us-west-2 mytenant myapp

# Or run individual checks:

# Check VPC capacity (replace ${AWS_REGION} with your region)
aws ec2 describe-vpcs --region ${AWS_REGION} --query 'length(Vpcs)' --output text

# Check NAT Gateway capacity
aws ec2 describe-nat-gateways --region ${AWS_REGION} \
    --filter "Name=state,Values=available" \
    --query 'length(NatGateways)' --output text

# Check EKS cluster status
aws eks describe-cluster --name ${TENANT}-workload --region ${AWS_REGION} \
    --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND"

# Check instance type availability in region
aws ec2 describe-instance-type-offerings --region ${AWS_REGION} \
    --location-type availability-zone \
    --filters "Name=instance-type,Values=t3.large" \
    --query 'InstanceTypeOfferings[*].Location' --output table

# Check node capacity on existing cluster
kubectl get nodes -o custom-columns=NAME:.metadata.name,CAPACITY:.status.capacity.pods,ALLOCATED:.status.allocatable.pods
```

### Common Capacity Issues

| Issue | Symptom | Solution |
|-------|---------|----------|
| VPC limit reached | "VpcLimitExceeded" error | Delete unused VPCs or request limit increase |
| NAT Gateway limit | "NatGatewayLimitExceeded" | Delete unused NAT Gateways or request increase |
| EIP limit | "AddressLimitExceeded" | Release unused EIPs or request increase |
| Instance type unavailable | Node group creation fails | Use different instance type or different AZ |
| EKS version unavailable | Cluster creation fails | Use available version from `aws eks describe-addon-versions` |
| Pod capacity exhausted | Pods stuck in Pending | Scale node group or use larger instance type |

---

## MENTAL MODEL: 4-Layer GitOps Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: INFRASTRUCTURE (One-time, Terraform)                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  â€¢ VPC, Subnets, NAT Gateway, Internet Gateway                              â”‚
â”‚  â€¢ EKS Cluster + Node Groups                                                â”‚
â”‚  â€¢ ECR Repositories                                                         â”‚
â”‚  â€¢ IAM Roles (cluster role, node role, OIDC/IRSA)                          â”‚
â”‚                                                                             â”‚
â”‚  When: Once per cluster, rarely changes                                     â”‚
â”‚  How:  terraform apply                                                      â”‚
â”‚  GitOps: terraform.yaml workflow (plan on PR, apply on merge)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: PLATFORM (One-time per cluster, kubectl/Helm)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚  â€¢ ArgoCD Installation (namespace + manifests)                              â”‚
â”‚  â€¢ ExternalDNS Installation + IRSA Role                                     â”‚
â”‚  â€¢ AWS Load Balancer Controller (optional)                                  â”‚
â”‚  â€¢ cert-manager (optional)                                                  â”‚
â”‚  â€¢ Cluster Autoscaler (optional)                                            â”‚
â”‚                                                                             â”‚
â”‚  When: Once after cluster creation                                          â”‚
â”‚  How:  kubectl apply or Helm install                                        â”‚
â”‚  GitOps: App-of-Apps pattern for infrastructure                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: ENVIRONMENTS (Per environment/tenant)                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚  â€¢ Namespaces (dev, staging, prod per tenant)                               â”‚
â”‚  â€¢ RBAC (Roles, RoleBindings)                                               â”‚
â”‚  â€¢ Resource Quotas                                                          â”‚
â”‚  â€¢ LimitRanges                                                              â”‚
â”‚  â€¢ Network Policies                                                         â”‚
â”‚                                                                             â”‚
â”‚  When: Once per environment/tenant                                          â”‚
â”‚  How:  ArgoCD ApplicationSet                                                â”‚
â”‚  GitOps: applicationset-environments.yaml                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 4: APPLICATIONS (Per application, on every push)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚  â€¢ CI/CD Workflow (GitHub Actions)                                          â”‚
â”‚  â€¢ Dockerfiles                                                              â”‚
â”‚  â€¢ Kustomize base + overlays                                                â”‚
â”‚  â€¢ ArgoCD Application                                                       â”‚
â”‚                                                                             â”‚
â”‚  When: On every git push                                                    â”‚
â”‚  How:  GitHub Actions â†’ ECR â†’ Update Kustomize â†’ ArgoCD sync               â”‚
â”‚  GitOps: ArgoCD Application watching git repo                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Layered Approach?

| Concern | Traditional | GitOps Layers |
|---------|-------------|---------------|
| **Separation** | All mixed together | Clear boundaries between infra, platform, and apps |
| **Lifecycle** | Everything deploys together | Each layer has its own lifecycle |
| **Ownership** | Single team owns everything | Platform team vs App teams |
| **Change Rate** | All changes same velocity | Infra: yearly, Platform: monthly, Apps: daily |
| **Blast Radius** | One change affects all | Changes isolated to layer |

---

## Template Directory Structure

```
repo-root/
â”‚
â”œâ”€â”€ .github/workflows/                    # âš ï¸  CRITICAL: At REPO ROOT, NOT in deployment folder!
â”‚   â””â”€â”€ ci-cd-{app}.yaml                  # CI/CD workflow (named per-app for multi-app repos)
â”‚
â”œâ”€â”€ {app}-opsera/                         # Deployment configs folder (e.g., myapp-opsera/)
â”‚   â”œâ”€â”€ DEPLOYMENT-CONTEXT.md             # Deployment tracking document
â”‚   â”œâ”€â”€ .deployment-state.json            # Resume/restart state tracking
â”‚   â”‚
â”‚   â”œâ”€â”€ argocd/                           # ArgoCD Application (FLAT)
â”‚   â”‚   â””â”€â”€ application.yaml              # ArgoCD Application manifest
â”‚   â”‚
â”‚   â”œâ”€â”€ terraform/                        # Infrastructure (app-scoped ECR)
â”‚   â”‚   â”œâ”€â”€ main.tf                       # ECR repos, tags
â”‚   â”‚   â”œâ”€â”€ variables.tf                  # App/tenant variables
â”‚   â”‚   â””â”€â”€ .gitignore                    # Exclude .terraform/, *.tfstate
â”‚   â”‚
â”‚   â””â”€â”€ k8s/                              # Kubernetes manifests (Kustomize)
â”‚       â”œâ”€â”€ base/                         # Base manifests
â”‚       â”‚   â”œâ”€â”€ backend-deployment.yaml
â”‚       â”‚   â”œâ”€â”€ backend-service.yaml
â”‚       â”‚   â”œâ”€â”€ frontend-deployment.yaml
â”‚       â”‚   â”œâ”€â”€ frontend-service.yaml
â”‚       â”‚   â””â”€â”€ kustomization.yaml
â”‚       â””â”€â”€ overlays/
â”‚           â””â”€â”€ dev/                      # Environment overlay
â”‚               â”œâ”€â”€ kustomization.yaml
â”‚               â””â”€â”€ namespace.yaml
â”‚
â”œâ”€â”€ backend/                              # Application source (separate from opsera/)
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ frontend/                             # Application source (separate from opsera/)
    â””â”€â”€ ...
```

**Key Points**:
- `{app}-opsera/` contains ONLY deployment configs (manifests, terraform, argocd)
- Application source code (backend/, frontend/) lives separately at repo root
- No `scripts/` folder - setup is handled by the skill
- No `gitops/` prefix - `argocd/` is flat at the root

### âš ï¸ CRITICAL: GitHub Actions Workflow Location

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  GITHUB ACTIONS WORKFLOW MUST BE AT REPO ROOT                                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  âœ… CORRECT: .github/workflows/ci-cd.yaml (at repo root)                      â•‘
â•‘  âŒ WRONG:   myapp-opsera/.github/workflows/ci-cd.yaml (inside folder)        â•‘
â•‘                                                                               â•‘
â•‘  GitHub Actions ONLY reads from: <repo-root>/.github/workflows/               â•‘
â•‘  Workflows inside subfolders are IGNORED and will NOT trigger!                â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Why This Matters**:
- GitHub only looks for workflow files at `<repo-root>/.github/workflows/`
- If you create the workflow inside `myapp-opsera/.github/workflows/`, it will NEVER run
- This is a common mistake that causes "workflow not triggering" issues

**Multi-App Repo Pattern**:
For repos with multiple apps, name workflows per-app:
```
.github/workflows/
â”œâ”€â”€ ci-cd-app1.yaml      # Triggers on app1-opsera branch
â”œâ”€â”€ ci-cd-app2.yaml      # Triggers on app2-opsera branch
â””â”€â”€ ci-cd-app3.yaml      # Triggers on app3-opsera branch
```

**Workflow Trigger Configuration**:
```yaml
# .github/workflows/ci-cd-myapp.yaml
name: CI/CD - myapp

on:
  push:
    branches:
      - myapp-opsera      # Only triggers on this app's branch
    paths:
      - 'myapp-opsera/**' # Only triggers on changes to this app's configs
      - 'backend/**'      # Or app source changes
      - 'frontend/**'
```

---

## Infrastructure Values

**All values are derived from user inputs. Use SAME region for all resources:**

| Parameter | Value | Notes |
|-----------|-------|-------|
| `aws_account_id` | `$(aws sts get-caller-identity --query Account --output text)` | Auto-detected from AWS credentials |
| `aws_region` | `${AWS_REGION}` | User-provided, use for EKS, ECR, AND ACM |
| `eks_cluster` | `${TENANT}-argocd` / `${TENANT}-workload-${ENV}` | Derived from tenant name |
| `route53_domain` | `opsera-labs.com` | Default hosted zone |
| `hosted_zone_id` | Look up with: `aws route53 list-hosted-zones` | Query at runtime |
| `acm_cert_arn` | Look up with: `aws acm list-certificates --region ${AWS_REGION}` | Must be in SAME region as EKS |

**IMPORTANT**: ACM certificates are region-specific! Always create/use cert in same region as EKS cluster (Fix #44).

---

## Quick Start: Deploy an Application

> **IMPORTANT**: You MUST complete "Step 0: MANDATORY FIRST STEP" above before proceeding!

### Step 1: Confirm Context is Initialized

Verify these are complete (from Step 0):
- âœ… `TENANT`, `APP_IDENTIFIER`, and `AWS_REGION` collected
- âœ… Branch `{app}-opsera` created and pushed
- âœ… Folder `{app}-opsera/` created with structure
- âœ… Initial commit pushed to remote

```bash
# Verify you're on the correct branch
git branch --show-current  # Should show: myapp-opsera (example)

# Verify folder exists
ls -la *-opsera/  # Should show your deployment configs folder
```

### Step 2: Configure Deployment

```bash
# The {app}-opsera folder contains deployment configs only:
# - argocd/application.yaml
# - terraform/main.tf
# - k8s/base/*.yaml
# - k8s/overlays/dev/*.yaml
# - .github/workflows/ci-cd.yaml

# Application source code lives separately (repo root or different repo)
```

### Step 3: Deploy

```bash
# Commit and push deployment configs
git add ${DEPLOYMENT_NAME}/
git commit -m "Add ${DEPLOYMENT_NAME} deployment configs"
git push origin ${DEPLOYMENT_NAME}
```

### Step 4: Result

```
CI/CD Pipeline:                    GitOps:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€
Build & Test  â”€â”€â–º  Docker Build    ArgoCD detects change
Security Scan â”€â”€â–º  ECR Push        Syncs to cluster
Update k8s/   â”€â”€â–º  Git Commit      Creates namespace
                                   Deploys pods
                                   Creates NLB
                                   ExternalDNS â†’ Route53

Result: https://<app>-<env>.agents.opsera-labs.com
```

### Deployment Completion Banner

When deployment is successful, display this banner:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                          â•‘
â•‘    â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘                         â•‘
â•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                         â•‘
â•‘     â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•                         â•‘
â•‘                                                                               â•‘
â•‘                    âœ… DEPLOYMENT SUCCESSFUL                                   â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  Application: <APP_IDENTIFIER>-<ENVIRONMENT>                                  â•‘
â•‘  Namespace:   <APP_IDENTIFIER>-<ENVIRONMENT>                                  â•‘
â•‘  ArgoCD App:  <APP_IDENTIFIER>-argo-<ENVIRONMENT>                            â•‘
â•‘                                                                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚  ğŸŒ ENDPOINT: https://<APP_IDENTIFIER>-<ENV>.agents.opsera-labs.com     â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                               â•‘
â•‘  Resources Created:                                                           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â€¢ ECR Repositories: <app>-backend, <app>-frontend                           â•‘
â•‘  â€¢ Kubernetes Namespace: <app>-<env>                                         â•‘
â•‘  â€¢ ArgoCD Application: <app>-argo-<env>                                      â•‘
â•‘  â€¢ LoadBalancer (NLB) with HTTPS/SSL                                         â•‘
â•‘  â€¢ Route53 DNS Record (via ExternalDNS)                                      â•‘
â•‘                                                                               â•‘
â•‘  Quick Commands:                                                              â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  â€¢ Check pods:    kubectl get pods -n <namespace>                            â•‘
â•‘  â€¢ Check logs:    kubectl logs -l app=<app>-backend -n <namespace>           â•‘
â•‘  â€¢ ArgoCD status: kubectl get app <app>-argo-<env> -n argocd                 â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          Powered by Opsera DevOps Platform
```

### Folder Structure After Setup

```
repo-root/
â”œâ”€â”€ myapp-opsera/                    # Deployment configs ({app}-opsera)
â”‚   â”œâ”€â”€ DEPLOYMENT-CONTEXT.md        # Tracks all config
â”‚   â”œâ”€â”€ argocd/                      # ArgoCD Application (FLAT)
â”‚   â”‚   â””â”€â”€ application.yaml
â”‚   â”œâ”€â”€ terraform/                   # ECR repos (app-scoped)
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â””â”€â”€ .gitignore
â”‚   â”œâ”€â”€ k8s/
â”‚   â”‚   â”œâ”€â”€ base/                    # Kustomize base
â”‚   â”‚   â””â”€â”€ overlays/dev/            # Dev overlay
â”‚   â””â”€â”€ .github/workflows/
â”‚       â””â”€â”€ ci-cd.yaml               # GitHub Actions
â”‚
â”œâ”€â”€ backend/                         # App source (SEPARATE from opsera folder)
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/                        # App source (SEPARATE from opsera folder)
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ README.md
```

**Note**: Application source code (`backend/`, `frontend/`) lives at repo root, NOT inside the `{app}-opsera/` folder.

---

## Opsera Workbench Auto-Generation

**IMPORTANT**: Generate the Opsera Workbench during Step 0 bootstrap to provide local monitoring.

### What is Opsera Workbench?

A local, dockerized dashboard that monitors GitHub Actions workflows in real-time:
- Visual pipeline topology with stage status
- Real-time workflow and job monitoring via GitHub API
- Endpoint health checking
- Auto-refresh every 10 seconds
- Works with any GitHub Personal Access Token

### Auto-Generate During Bootstrap

Add this to your Step 0.2 initialization script:

```bash
# Create Opsera Workbench directory
mkdir -p .opsera/deployment-monitor

# Generate config.json
cat > .opsera/deployment-monitor/config.json << EOF
{
  "app": {
    "name": "${APP_IDENTIFIER}",
    "environment": "${ENVIRONMENT}",
    "tenant": "${TENANT}"
  },
  "github": {
    "owner": "${GITHUB_ORG}",
    "repo": "${GITHUB_REPO}",
    "branch": "${DEPLOYMENT_NAME}"
  },
  "endpoint": "https://${APP_IDENTIFIER}-${ENVIRONMENT}.agents.opsera-labs.com",
  "protocol": "https",
  "stages": [
    {
      "id": "terraform",
      "name": "Terraform Infrastructure",
      "workflow": "01-deploy-infrastructure.yaml",
      "job": "Terraform Infrastructure",
      "estimatedTime": "10-15 min"
    },
    {
      "id": "cluster-setup",
      "name": "Setup EKS Cluster",
      "workflow": "01-deploy-infrastructure.yaml",
      "job": "Setup EKS Cluster",
      "estimatedTime": "5-8 min"
    },
    {
      "id": "docker-build",
      "name": "Build & Push Images",
      "workflow": "02-build-deploy.yaml",
      "job": "Build & Push Images",
      "estimatedTime": "3-5 min"
    },
    {
      "id": "k8s-deploy",
      "name": "Update K8s Manifests",
      "workflow": "02-build-deploy.yaml",
      "job": "Update K8s Manifests",
      "estimatedTime": "1-2 min"
    },
    {
      "id": "verify",
      "name": "Verify Deployment",
      "workflow": "02-build-deploy.yaml",
      "job": "Verify Deployment",
      "estimatedTime": "2-3 min"
    }
  ],
  "refreshInterval": 10000,
  "version": "1.0.0"
}
EOF

# Generate Dockerfile
cat > .opsera/deployment-monitor/Dockerfile << 'EOF'
FROM nginx:alpine

LABEL maintainer="Opsera DevOps Platform"
LABEL description="Opsera Workbench - Real-time GitOps tracking dashboard"
LABEL version="1.0.0"

COPY index.html /usr/share/nginx/html/
COPY config.json /usr/share/nginx/html/

RUN echo 'server { \
    listen 80; \
    server_name localhost; \
    root /usr/share/nginx/html; \
    index index.html; \
    location / { \
        try_files $uri $uri/ /index.html; \
    } \
}' > /etc/nginx/conf.d/default.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
EOF

# Generate docker-compose.yml
cat > .opsera/deployment-monitor/docker-compose.yml << EOF
version: '3.8'
services:
  workbench:
    build: .
    ports:
      - "8888:80"
    container_name: opsera-workbench-${APP_IDENTIFIER}
EOF

# Generate start.sh
cat > .opsera/deployment-monitor/start.sh << 'EOF'
#!/bin/bash
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                                                                               â•‘"
echo "â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                          â•‘"
echo "â•‘    â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—                         â•‘"
echo "â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                         â•‘"
echo "â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘                         â•‘"
echo "â•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                         â•‘"
echo "â•‘     â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•                         â•‘"
echo "â•‘                                                                               â•‘"
echo "â•‘                         â”â”â” OPSERA WORKBENCH â”â”â”                             â•‘"
echo "â•‘                                                                               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
cd "$(dirname "$0")"
docker-compose up --build -d
echo ""
echo "Opsera Workbench started at: http://localhost:8888"
echo ""
echo "To stop: docker-compose down"
EOF
chmod +x .opsera/deployment-monitor/start.sh
```

**Note**: The `index.html` file is ~700 lines and should be copied from the skill's template or a reference deployment.

### Starting Opsera Workbench

```bash
cd .opsera/deployment-monitor
./start.sh
# Or directly:
docker-compose up --build -d
```

Open http://localhost:8888 and enter your GitHub Personal Access Token to connect.

### Features

- **Real-time Stage Tracking**: See which CI/CD stages are pending, running, completed, or failed
- **Visual Pipeline Topology**: ASCII art diagram showing infrastructure and deployment flow
- **Elapsed Time**: Track how long the deployment has been running
- **Endpoint Health Check**: Automatically verifies your endpoint is responding
- **Logs**: View deployment activity log (newest first)
- **Auto-refresh**: Updates every 10 seconds (configurable)

---

## Layer-by-Layer Setup Guide

### LAYER 1: Infrastructure (Terraform)

**When**: Once per cluster (rarely changes)

```hcl
# terraform/aws/main.tf
variable "tenant" { description = "Tenant name" }
variable "environment" { description = "Environment (dev/staging/prod)" }
variable "aws_region" { description = "AWS region for all resources" }

module "vpc_eks" {
  source       = "./modules/vpc-eks"
  cluster_name = "${var.tenant}-${var.environment}"
  environment  = var.environment
  aws_region   = var.aws_region  # Same region for everything
  vpc_cidr     = "10.0.0.0/16"
}
```

```bash
cd terraform/aws
terraform init
terraform plan
terraform apply
```

**Outputs**:
- Cluster endpoint
- OIDC provider ARN (for IRSA)
- External DNS IAM role ARN
- ECR repository URLs

---

### LAYER 2: Platform (kubectl/Helm)

**When**: Once after cluster creation

```bash
# Run the platform setup script
./scripts/platform-setup.sh

# Or manually:
# 1. Install ArgoCD
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# 2. Install ExternalDNS (with IRSA)
kubectl apply -f templates/platform/external-dns.yaml

# 3. (Optional) Install App-of-Apps
kubectl apply -f templates/platform/app-of-apps.yaml
```

**Verify**:
```bash
# ArgoCD
kubectl get pods -n argocd
kubectl port-forward svc/argocd-server -n argocd 8080:443

# ExternalDNS
kubectl logs -l app=external-dns -n kube-system
```

---

### LAYER 3: Environments (ArgoCD ApplicationSet)

**When**: Once per environment

Use ApplicationSet for multi-environment deployments:

```yaml
# gitops/applicationset-environments.yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: my-app-environments
spec:
  generators:
    - list:
        elements:
          - env: dev
            namespace: my-app-dev
          - env: staging
            namespace: my-app-staging
          - env: prod
            namespace: my-app-prod
  template:
    # ... see templates/platform/applicationset-environments.yaml.template
```

---

### LAYER 4: Applications (CI/CD + ArgoCD)

**When**: On every git push

```yaml
# .github/workflows/ci-cd.yaml
name: CI/CD Pipeline

on:
  push:
    branches: [my-app]

env:
  TENANT: ${{ vars.TENANT }}                    # Set in GitHub repo variables
  APP_NAME: ${{ vars.APP_IDENTIFIER }}
  AWS_REGION: ${{ vars.AWS_REGION }}            # Single region for EKS, ECR, ACM
  CLUSTER_NAME: ${{ vars.TENANT }}-argocd
  ENDPOINT: https://${{ vars.APP_IDENTIFIER }}-${{ vars.ENVIRONMENT }}.agents.opsera-labs.com

jobs:
  build-test:
    # Build and test application

  security-scan:
    # Trivy scanning

  docker-build:
    # Build and push to ECR

  update-manifests:
    # Update kustomization.yaml with new image tag

  verify-deployment:
    # Wait for ArgoCD sync and verify endpoint
```

---

## Custom Domain Setup (GoDaddy/Other Registrar + Route53 + ACM)

When deploying to a **new AWS account** or using a **custom domain** (not pre-configured), follow these steps:

### Step 1: Create Route53 Hosted Zone

```bash
# Create hosted zone for your domain
aws route53 create-hosted-zone \
  --name yourdomain.com \
  --caller-reference "$(date +%s)"

# Get the hosted zone ID and name servers
aws route53 list-hosted-zones --query 'HostedZones[*].[Id,Name]'
aws route53 get-hosted-zone --id <ZONE_ID> --query 'DelegationSet.NameServers'
```

### Step 2: Update Domain Registrar Name Servers

**GoDaddy (Manual - API often has access issues):**
1. Go to https://dcc.godaddy.com/control/yourdomain.com/dns
2. Click **Nameservers** â†’ **Change**
3. Select **"I'll use my own nameservers"**
4. Enter the 4 AWS nameservers (e.g., `ns-806.awsdns-36.net`)
5. Click **Save**

**Other Registrars:** Update NS records in your registrar's DNS settings.

### Step 3: Request ACM Certificate

```bash
# Request wildcard certificate
CERT_ARN=$(aws acm request-certificate \
  --domain-name "*.yourdomain.com" \
  --subject-alternative-names "yourdomain.com" \
  --validation-method DNS \
  --query 'CertificateArn' --output text)

# Get DNS validation records
aws acm describe-certificate --certificate-arn $CERT_ARN \
  --query 'Certificate.DomainValidationOptions[0].ResourceRecord'
```

### Step 4: Create DNS Validation Record in Route53

```bash
# Get validation record details
VALIDATION=$(aws acm describe-certificate --certificate-arn $CERT_ARN \
  --query 'Certificate.DomainValidationOptions[0].ResourceRecord')

# Create the CNAME record for validation
aws route53 change-resource-record-sets --hosted-zone-id <ZONE_ID> --change-batch '{
  "Changes": [{
    "Action": "UPSERT",
    "ResourceRecordSet": {
      "Name": "<validation_name>",
      "Type": "CNAME",
      "TTL": 300,
      "ResourceRecords": [{"Value": "<validation_value>"}]
    }
  }]
}'
```

### Step 5: Wait for Certificate Validation

```bash
# Check certificate status (wait for ISSUED)
aws acm describe-certificate --certificate-arn $CERT_ARN \
  --query 'Certificate.Status'

# Typically takes 5-30 minutes after DNS propagates
# DNS propagation can take 15 minutes to 48 hours
```

### Step 6: Create App DNS Record

```bash
# Point your app subdomain to the LoadBalancer
aws route53 change-resource-record-sets --hosted-zone-id <ZONE_ID> --change-batch '{
  "Changes": [{
    "Action": "UPSERT",
    "ResourceRecordSet": {
      "Name": "myapp.yourdomain.com",
      "Type": "CNAME",
      "TTL": 60,
      "ResourceRecords": [{"Value": "<loadbalancer-hostname>"}]
    }
  }]
}'
```

### Step 7: Update Frontend Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-frontend
  annotations:
    external-dns.alpha.kubernetes.io/hostname: myapp.yourdomain.com
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "<your-cert-arn>"
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443"
spec:
  type: LoadBalancer
  ports:
    - name: http
      port: 80
      targetPort: 3000
    - name: https
      port: 443
      targetPort: 3000
```

### Critical: Certificate Must Be ISSUED

The LoadBalancer **cannot create HTTPS listener** until certificate status is `ISSUED`. If you see:
```
UnsupportedCertificate: The certificate must have a fully-qualified domain name
```

This means the certificate is still `PENDING_VALIDATION`. Wait for validation to complete.

### DNS Propagation Verification

```bash
# Check via Google DNS (faster than local)
dig yourdomain.com @8.8.8.8 NS +short
dig myapp.yourdomain.com @8.8.8.8 +short
dig _acm-validation.yourdomain.com @8.8.8.8 CNAME +short
```

---

## New AWS Account Full Setup

When deploying to a **completely new AWS account**, you need to provision everything:

### 1. Create Terraform Infrastructure

```hcl
# terraform/main.tf
variable "tenant" { description = "Tenant name (e.g., mycompany)" }
variable "environment" { description = "Environment (dev/staging/prod)" }
variable "aws_region" { description = "AWS region - use SAME for EKS, ECR, ACM" }

locals {
  cluster_name = "${var.tenant}-${var.environment}"
}

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"
  name    = "${local.cluster_name}-vpc"
  cidr    = "10.0.0.0/16"
  azs     = ["${var.aws_region}a", "${var.aws_region}b"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24"]
  enable_nat_gateway = true
  single_nat_gateway = true
}

module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"
  cluster_name    = local.cluster_name
  cluster_version = "1.29"
  vpc_id          = module.vpc.vpc_id
  subnet_ids      = module.vpc.private_subnets
  enable_irsa     = true

  # CRITICAL: Allow cluster creator to access cluster (Fix #40)
  enable_cluster_creator_admin_permissions = true

  eks_managed_node_groups = {
    general = {
      instance_types = ["t3.large"]  # t3.large = 35 pods (Fix #13)
      min_size       = 2
      max_size       = 4
      desired_size   = 2
    }
  }
}
```

### 2. Create ECR Repositories

```bash
# Use SAME region as EKS cluster!
aws ecr create-repository --repository-name ${APP_IDENTIFIER}-backend --region ${AWS_REGION}
aws ecr create-repository --repository-name ${APP_IDENTIFIER}-frontend --region ${AWS_REGION}
```

### 3. Install Platform Components

```bash
# Connect to cluster (use same region as EKS)
aws eks update-kubeconfig --name ${TENANT}-${ENVIRONMENT} --region ${AWS_REGION}

# Install ArgoCD
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Install ExternalDNS (update domain-filter for your domain)
kubectl apply -f external-dns.yaml
```

### 4. Update ExternalDNS Domain Filter

When using a custom domain, update ExternalDNS:

```bash
kubectl patch deployment external-dns -n kube-system --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/args/2", "value": "--domain-filter=yourdomain.com"}]'
```

---

## VERIFIED FIXES (55 Production Learnings)

### Critical Fixes Applied to All Templates

| # | Issue | Root Cause | Fix |
|---|-------|------------|-----|
| 1 | Python `ModuleNotFoundError` | `pip --user` installs to root's home | Use `--prefix=/install` |
| 2 | Container temp directory error | `readOnlyRootFilesystem: true` | Add emptyDir for `/tmp` |
| 3 | Nginx crash `Read-only file system` | Missing cache directories | Add emptyDir volumes for `/var/cache/nginx`, `/var/run` |
| 4 | Git push permission denied | Missing workflow permissions | Add `permissions: contents: write` |
| 5 | ArgoCD repository not found | Private repo, no credentials | Create repo secret with correct label |
| 6 | Multi-region ECR/EKS mismatch | ECR in different region than EKS | Use SAME region for EKS, ECR, and ACM |
| 7 | npm ci fails | Missing package-lock.json | Always commit package-lock.json |
| 8 | TypeScript build fails | Unused imports in strict mode | Clean imports before commit |
| 9 | HTTPS not working | Manual TLS listener | Use Service annotations for NLB SSL |
| 10 | Frontend API URL hardcoded | localhost in production | Use `import.meta.env.PROD` pattern |
| 11 | ImagePullBackOff | SHA mismatch (short vs full) | Use consistent SHORT SHA (7 chars) |
| 12 | Orphaned ArgoCD apps | Deleted namespace without app | Delete ArgoCD app BEFORE namespace |
| 13 | Pods Pending "too many pods" | Node instance type limits | Use t3.large (35 pods) or larger |
| 14 | ExternalDNS AccessDenied | Missing IRSA role | Create IRSA with OIDC trust |
| 15 | NLB not responding | Propagation delay | Wait 2-5 minutes for NLB |
| 16 | Terraform state lost | Local backend | Use S3 backend |
| 17 | IAM role exists error | Roles are global | Import before apply |
| 18 | ECR repo exists | Previous deployment | Check before create |
| 19 | ACM cert PENDING_VALIDATION | DNS not propagated | Wait for nameserver propagation |
| 20 | NLB HTTPS listener fails | Certificate not ISSUED | Wait for ACM validation complete |
| 21 | Nginx upstream not found | Wrong backend service name | Match nginx.conf proxy_pass to k8s service name |
| 22 | LoadBalancer stuck pending | Old cert reference | Delete and recreate service |
| 23 | DNS not resolving | Local DNS cache | Flush DNS or test via @8.8.8.8 |
| 24 | GoDaddy API ACCESS_DENIED | API restrictions | Use manual UI nameserver update |
| 25 | Certificate account mismatch | Cert in wrong account | Create new cert in correct account |
| 26 | Route53 zone not found | Zone in different account | Create hosted zone in target account |
| 27 | LoadBalancer hostname changed | Service recreated | Update Route53 CNAME record |
| 28 | NLB provisioning state | Just created | Wait 2-5 min for active state |
| 29 | Image tag re-tagging needed | ECR only has full SHA | Use `aws ecr put-image` to add short tag |
| 30 | Git push rejected | CI/CD auto-commits | Use `git pull --rebase` first |
| 31 | ArgoCD SSH_AUTH_SOCK error | SSH URL with HTTPS secret | Use HTTPS URL in ArgoCD Application |
| 32 | Backend pod CrashLoopBackOff | Missing /health endpoint | Add `/health` endpoint to backend |
| 33 | Frontend 502 Bad Gateway | CORS not configured for production | Add production URL to CORS origins |
| 34 | Terraform provider binaries committed | Missing .gitignore | Add `.terraform/`, `*.tfstate*` to .gitignore |
| 35 | ACM certificate not found | Wrong cert ARN (typo/mismatch) | Verify cert ARN with `aws acm list-certificates` |
| 36 | Old pods blocking new deployments | Stuck terminating pods | Force delete with `--force --grace-period=0` |
| 37 | ExternalDNS CrashLoopBackOff | Missing RBAC ClusterRole/ClusterRoleBinding | Add ClusterRole with services/endpoints/pods/nodes/ingresses permissions |
| 38 | ExternalDNS not creating DNS records | domain-filter uses subdomain not parent zone | Use parent zone (opsera-labs.com not agents.opsera-labs.com) |
| 39 | ExternalDNS AccessDenied for Route53 | IRSA role ARN empty in ServiceAccount | Add fallback to get role ARN from Terraform output or hardcode |
| 40 | EKS cluster access denied after creation | Cluster creator not in aws-auth | Add `enable_cluster_creator_admin_permissions = true` in Terraform |
| 41 | GHA workflow not triggering | Workflow in deployment folder, not repo root | Create workflow at `<repo>/.github/workflows/`, NOT inside deployment folder |
| 42 | Deployment fails mid-way, can't resume | No state tracking, not idempotent | Use `.deployment-state.json` for tracking, all ops check-before-create |
| 43 | VPC limit exceeded during creation | No pre-flight capacity checks | Run pre-flight checks before infrastructure creation |
| 44 | ACM certificate not found (region mismatch) | ACM certs are region-specific, cert in us-west-1 but EKS in us-west-2 | Create ACM certificate in SAME region as EKS cluster |
| 45 | Cross-cluster ArgoCD registration timeout | Clusters in private subnets can't communicate | Use VPC peering, or deploy in-cluster to ArgoCD cluster |
| 46 | ExternalDNS AccessDenied on different cluster | IRSA trust policy only trusts original OIDC | Update IAM trust policy to include ALL cluster OIDC providers |
| 47 | ACM validation instant for existing domain | DNS validation CNAME already exists from prior cert | No action needed - validation is domain-based, reuses existing record |
| 48 | LoadBalancer not updating after cert change | NLB doesn't refresh certificate annotation | Delete service, let ArgoCD recreate it with new cert |
| 49 | VPC deletion fails with dependencies | Resources have circular dependencies | Delete in order: NAT GW â†’ SG rules â†’ SGs â†’ Subnets â†’ IGW â†’ RTBs â†’ VPC |
| 50 | Terraform outputs not available in GHA jobs | Local backend doesn't share state between jobs | Use S3 backend or pass outputs via GHA artifacts/outputs |
| 51 | Nginx permission denied on port 80 | Non-root user (uid 101) can't bind to privileged ports | Use port 8080 in nginx.conf, update deployment and service targetPort |
| 52 | GitHub Actions commit loop | Workflow updates k8s files which triggers new run | Exclude `k8s/**` from path triggers, add `[skip ci]` to automated commits |
| 53 | ArgoCD "authentication required" | Empty GH_PAT_TOKEN during repo secret creation | Verify PAT is not empty before creating secret, recreate with valid token |
| 54 | ExternalDNS DNS record loop | Multiple clusters with same `txt-owner-id` | Use unique owner ID per cluster (e.g., `{tenant}-external-dns`) |
| 55 | ExternalDNS not persisting records | Owner ID conflict with other clusters | Manually create Route53 A record with alias to NLB |
| 68 | ECR repos don't exist when GHA runs | Race condition: Git push triggers GHA before ECR created | Create ECR repos BEFORE git push, or re-trigger workflow after ECR creation |

### Detailed Fixes

#### 1. Python Dockerfile - Module Path

```dockerfile
# CORRECT - works with non-root user
FROM python:3.12-slim AS builder
RUN pip install --no-cache-dir --prefix=/install -r requirements.txt

FROM python:3.12-slim
COPY --from=builder /install /usr/local
```

#### 3. Nginx Frontend - emptyDir Volumes

```yaml
volumes:
  - name: cache
    emptyDir: {}
  - name: run
    emptyDir: {}
  - name: tmp
    emptyDir: {}
volumeMounts:
  - name: cache
    mountPath: /var/cache/nginx
  - name: run
    mountPath: /var/run
  - name: tmp
    mountPath: /tmp
```

#### 9. NLB HTTPS via Service Annotations

```yaml
apiVersion: v1
kind: Service
metadata:
  name: app-frontend
  annotations:
    external-dns.alpha.kubernetes.io/hostname: app.agents.opsera-labs.com
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:..."
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443"
spec:
  type: LoadBalancer
  ports:
    - name: http
      port: 80
      targetPort: 3000
    - name: https
      port: 443
      targetPort: 3000
```

#### 21. Nginx Upstream Not Found

When frontend nginx crashes with `host not found in upstream`:

```nginx
# WRONG - service name doesn't match k8s service
location /api {
    proxy_pass http://old-app-backend:8000;  # Wrong name!
}

# CORRECT - must match k8s service name exactly
location /api {
    proxy_pass http://myapp-backend:8000;  # Matches service name
}
```

#### 22. LoadBalancer Stuck with Old Certificate

When LoadBalancer fails with `CertificateNotFound` after cert change:

```bash
# Delete and recreate the service to force new LoadBalancer
kubectl delete svc myapp-frontend -n myapp
kubectl apply -f k8s/base/frontend-service.yaml -n myapp

# Wait for new LoadBalancer
kubectl get svc myapp-frontend -n myapp -w

# Update DNS to new LoadBalancer hostname
aws route53 change-resource-record-sets --hosted-zone-id <ZONE_ID> --change-batch '{
  "Changes": [{
    "Action": "UPSERT",
    "ResourceRecordSet": {
      "Name": "myapp.yourdomain.com",
      "Type": "CNAME",
      "TTL": 60,
      "ResourceRecords": [{"Value": "<new-lb-hostname>"}]
    }
  }]
}'
```

#### 28. NLB Provisioning State

NLB takes 2-5 minutes to become active after creation:

```bash
# Check NLB state
aws elbv2 describe-load-balancers \
  --query 'LoadBalancers[*].[LoadBalancerName,State.Code,DNSName]' \
  --output table

# Wait for active state
for i in {1..20}; do
  STATE=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[0].State.Code' --output text)
  echo "Check $i/20: State = $STATE"
  if [ "$STATE" = "active" ]; then break; fi
  sleep 15
done
```

#### 29. ECR Image Re-tagging

When CI/CD pushes with full SHA but kustomization needs short SHA:

```bash
# Get manifest for full SHA image
MANIFEST=$(aws ecr batch-get-image \
  --repository-name myapp-backend \
  --image-ids imageTag=abc123def456... \
  --query 'images[0].imageManifest' --output text)

# Create new tag with short SHA
aws ecr put-image \
  --repository-name myapp-backend \
  --image-tag abc1234 \
  --image-manifest "$MANIFEST"
```

#### 31. ArgoCD SSH_AUTH_SOCK Error

When ArgoCD fails with `SSH_AUTH_SOCK not-specified`:

```yaml
# WRONG - SSH URL but HTTPS credentials in secret
spec:
  source:
    repoURL: git@github.com:org/repo.git  # SSH format

# CORRECT - Match URL format to credential type
spec:
  source:
    repoURL: https://github.com/org/repo.git  # HTTPS format
```

Also ensure the repo secret uses matching type:
```bash
# For HTTPS repos
kubectl create secret generic repo-myapp -n argocd \
  --from-literal=url=https://github.com/org/repo.git \
  --from-literal=username=<github-user-or-token-name> \
  --from-literal=password=<github-pat-token> \
  --from-literal=type=git \
  --dry-run=client -o yaml | kubectl apply -f -
kubectl label secret repo-myapp -n argocd argocd.argoproj.io/secret-type=repository --overwrite
```

#### 32. Backend /health Endpoint Missing

When backend pods restart with `CrashLoopBackOff` due to failing health probes:

```python
# FastAPI - Add health endpoint
@app.get("/health")
async def health():
    """Health check endpoint for Kubernetes probes"""
    return {"status": "healthy"}
```

```javascript
// Express.js - Add health endpoint
app.get('/health', (req, res) => {
  res.json({ status: 'healthy' });
});
```

Then configure deployment probes:
```yaml
spec:
  containers:
  - name: backend
    livenessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 10
      periodSeconds: 10
    readinessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 5
      periodSeconds: 5
```

#### 33. CORS Not Configured for Production

When frontend gets 502 Bad Gateway or CORS errors in production:

```python
# FastAPI - Include production URL
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",     # Vite dev
        "http://localhost:3000",     # Local nginx
        f"https://{APP_IDENTIFIER}-{ENVIRONMENT}.agents.opsera-labs.com"  # Production!
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

#### 34. Terraform State Files Committed

When Terraform providers and state get committed to git:

```bash
# Add to .gitignore BEFORE terraform init
cat > terraform/.gitignore << 'EOF'
# Terraform
.terraform/
*.tfstate
*.tfstate.*
.terraform.lock.hcl
*.tfplan

# Credentials (never commit)
*.pem
*.key
credentials.json
EOF

# If already committed, remove from tracking
git rm --cached -r terraform/.terraform
git rm --cached terraform/*.tfstate*
git rm --cached terraform/.terraform.lock.hcl
git commit -m "Remove terraform state from tracking"
```

#### 35. ACM Certificate ARN Wrong/Not Found

When LoadBalancer fails with `CertificateNotFound`:

```bash
# List all certificates in the region
aws acm list-certificates --region us-west-1 \
  --query 'CertificateSummaryList[*].[CertificateArn,DomainName,Status]' \
  --output table

# Get the CORRECT ARN (look for ISSUED status and matching domain)
# Common mistake: mixing up similar ARNs with different UUIDs

# Update service with correct ARN
kubectl edit svc myapp-frontend -n myapp
# Change: service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "correct-arn"
```

#### 36. Stuck Terminating Pods

When old pods are stuck in `Terminating` and blocking new deployments:

```bash
# Check for stuck pods
kubectl get pods -n myapp | grep Terminating

# Force delete (use sparingly - can cause data loss)
kubectl delete pod <pod-name> -n myapp --force --grace-period=0

# If namespace is stuck terminating
kubectl get namespace myapp -o json | jq '.spec.finalizers = []' | kubectl replace --raw "/api/v1/namespaces/myapp/finalize" -f -
```

#### 37. ExternalDNS CrashLoopBackOff - Missing RBAC

When ExternalDNS fails with `context deadline exceeded` or API server errors:

```yaml
# REQUIRED: ClusterRole and ClusterRoleBinding for ExternalDNS
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: external-dns
rules:
- apiGroups: [""]
  resources: ["services","endpoints","pods","nodes"]
  verbs: ["get","watch","list"]
- apiGroups: ["extensions","networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get","watch","list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: external-dns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-dns
subjects:
- kind: ServiceAccount
  name: external-dns
  namespace: kube-system
```

#### 38. ExternalDNS Not Creating DNS Records - Wrong Domain Filter

When ExternalDNS runs but doesn't create Route53 records:

```bash
# WRONG - using subdomain as filter
args:
- --domain-filter=agents.opsera-labs.com  # Won't match parent zone!

# CORRECT - use parent zone
args:
- --domain-filter=opsera-labs.com  # Matches the Route53 hosted zone
```

The domain-filter must match the Route53 hosted zone name, not the subdomain.

#### 39. ExternalDNS AccessDenied - Empty IRSA Role

When ExternalDNS shows `AccessDenied` for `route53:ListHostedZones`:

```yaml
# WRONG - empty IRSA role annotation
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: ""  # Empty!

# CORRECT - use Terraform output with fallback
annotations:
  eks.amazonaws.com/role-arn: ${EXTERNAL_DNS_ROLE:-arn:aws:iam::ACCOUNT_ID:role/TENANT-external-dns}
```

In GitHub Actions workflow:
```bash
EXTERNAL_DNS_ROLE=$(cd terraform && terraform output -raw external_dns_role_arn 2>/dev/null || \
  echo "arn:aws:iam::${AWS_ACCOUNT_ID}:role/${TENANT}-external-dns")
```

#### 40. EKS Access Denied After Cluster Creation

When `kubectl` commands fail with `Unauthorized` after Terraform creates cluster:

```hcl
# REQUIRED in Terraform EKS module
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"

  cluster_name    = var.cluster_name
  cluster_version = "1.29"

  # CRITICAL: Allow cluster creator to access cluster
  enable_cluster_creator_admin_permissions = true

  # ... rest of config
}
```

Without this, the IAM user/role that creates the cluster won't be able to access it.

#### 44. ACM Certificate Region Mismatch

ACM certificates are **region-specific**. A certificate in us-west-1 cannot be used by a LoadBalancer in us-west-2:

```bash
# WRONG - deploying to us-west-2 but using us-west-1 cert
service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:us-west-1:123:certificate/abc"

# CORRECT - cert must be in same region as EKS cluster
service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:us-west-2:123:certificate/xyz"

# Check certificates in target region
aws acm list-certificates --region us-west-2 \
  --query 'CertificateSummaryList[*].[CertificateArn,DomainName,Status]' --output table

# Request new certificate if needed (DNS validation reuses existing CNAME!)
aws acm request-certificate --domain-name "*.agents.opsera-labs.com" \
  --validation-method DNS --region us-west-2
```

**Key insight**: If the DNS validation CNAME already exists from a previous certificate request for the same domain, the new certificate validates instantly!

#### 45. Cross-Cluster ArgoCD Registration Timeout (Greenfield Pattern)

When both ArgoCD and workload clusters are in **private subnets**, `argocd cluster add` times out because clusters cannot communicate directly:

```
{\"level\":\"fatal\",\"msg\":\"rpc error: code = Unknown desc = error getting server version:
Get \\\"https://CLUSTER_ID.eks.amazonaws.com/version?timeout=32s\\\": dial tcp 10.0.1.79:443: i/o timeout\"}
```

**Solutions** (choose one):

1. **Use in-cluster deployment** (simplest) - Deploy app to ArgoCD cluster itself:
   ```yaml
   spec:
     destination:
       server: https://kubernetes.default.svc  # In-cluster, not workload cluster
       namespace: myapp-dev
   ```

2. **Use VPC Peering** - Connect the VPCs so clusters can communicate

3. **Use public EKS endpoints** - Change `endpoint_private_access = true, endpoint_public_access = true`

4. **Use AWS Transit Gateway** - For enterprise multi-VPC connectivity

**Recommendation**: For Greenfield pattern with private subnets, use **brownfield-embedded** instead (single cluster) to avoid this complexity.

#### 46. ExternalDNS IRSA Multi-Cluster Trust Policy

When ExternalDNS runs on a **different cluster** than originally configured, it fails with `AccessDenied` because the IRSA trust policy only trusts the original cluster's OIDC provider:

```bash
# Check current trust policy
aws iam get-role --role-name ${TENANT}-external-dns --query 'Role.AssumeRolePolicyDocument'

# Trust policy MUST include ALL cluster OIDC providers
cat > /tmp/trust-policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::ACCOUNT:oidc-provider/oidc.eks.REGION.amazonaws.com/id/ARGOCD_CLUSTER_OIDC"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "oidc.eks.REGION.amazonaws.com/id/ARGOCD_CLUSTER_OIDC:sub": "system:serviceaccount:kube-system:external-dns"
        }
      }
    },
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::ACCOUNT:oidc-provider/oidc.eks.REGION.amazonaws.com/id/WORKLOAD_CLUSTER_OIDC"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "oidc.eks.REGION.amazonaws.com/id/WORKLOAD_CLUSTER_OIDC:sub": "system:serviceaccount:kube-system:external-dns"
        }
      }
    }
  ]
}
EOF

# Update the trust policy
aws iam update-assume-role-policy --role-name ${TENANT}-external-dns --policy-document file:///tmp/trust-policy.json
```

#### 48. LoadBalancer Not Updating After Certificate Change

When you change the ACM certificate ARN in service annotations, the existing LoadBalancer does **NOT** automatically update. You must delete and recreate the service:

```bash
# Delete the service (this deletes the LoadBalancer)
kubectl delete svc myapp-frontend -n myapp-dev

# Force ArgoCD to sync and recreate
kubectl annotate application myapp-argo-dev -n argocd argocd.argoproj.io/refresh=hard --overwrite

# Wait for new LoadBalancer with correct cert
kubectl get svc myapp-frontend -n myapp-dev -w
```

#### 49. VPC Deletion Order (Circular Dependencies)

AWS VPC deletion fails if dependencies exist. Delete in this specific order:

```bash
VPC_ID="vpc-0123456789abcdef0"
REGION="us-west-2"

# 1. Delete NAT Gateways first (they block everything)
for nat in $(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" \
  --query 'NatGateways[*].NatGatewayId' --output text --region $REGION); do
  aws ec2 delete-nat-gateway --nat-gateway-id $nat --region $REGION
  echo "Waiting for NAT Gateway $nat to delete..."
  aws ec2 wait nat-gateway-deleted --nat-gateway-ids $nat --region $REGION 2>/dev/null || sleep 60
done

# 2. Release Elastic IPs
for eip in $(aws ec2 describe-addresses --filter "Name=domain,Values=vpc" \
  --query 'Addresses[*].AllocationId' --output text --region $REGION); do
  aws ec2 release-address --allocation-id $eip --region $REGION 2>/dev/null || true
done

# 3. Revoke Security Group rules (break circular dependencies!)
for sg in $(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" \
  --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text --region $REGION); do
  # Revoke all ingress/egress rules
  aws ec2 describe-security-groups --group-ids $sg --region $REGION \
    --query 'SecurityGroups[0].IpPermissions' --output json | \
    jq -c '.[]' | while read rule; do
      aws ec2 revoke-security-group-ingress --group-id $sg --ip-permissions "$rule" --region $REGION 2>/dev/null || true
    done
  aws ec2 describe-security-groups --group-ids $sg --region $REGION \
    --query 'SecurityGroups[0].IpPermissionsEgress' --output json | \
    jq -c '.[]' | while read rule; do
      aws ec2 revoke-security-group-egress --group-id $sg --ip-permissions "$rule" --region $REGION 2>/dev/null || true
    done
done

# 4. Delete Security Groups
for sg in $(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" \
  --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text --region $REGION); do
  aws ec2 delete-security-group --group-id $sg --region $REGION 2>/dev/null || true
done

# 5. Delete Subnets
for subnet in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
  --query 'Subnets[*].SubnetId' --output text --region $REGION); do
  aws ec2 delete-subnet --subnet-id $subnet --region $REGION
done

# 6. Detach and delete Internet Gateway
for igw in $(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
  --query 'InternetGateways[*].InternetGatewayId' --output text --region $REGION); do
  aws ec2 detach-internet-gateway --internet-gateway-id $igw --vpc-id $VPC_ID --region $REGION
  aws ec2 delete-internet-gateway --internet-gateway-id $igw --region $REGION
done

# 7. Delete Route Tables (except main)
for rtb in $(aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_ID" \
  --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text --region $REGION); do
  aws ec2 delete-route-table --route-table-id $rtb --region $REGION
done

# 8. Finally delete VPC
aws ec2 delete-vpc --vpc-id $VPC_ID --region $REGION
```

#### 50. Terraform Outputs Not Available Between GHA Jobs

When using Terraform local backend, outputs from one job are NOT available to subsequent jobs:

```yaml
# WRONG - subsequent job can't read terraform output
jobs:
  terraform:
    steps:
      - run: terraform apply

  setup-externaldns:
    needs: terraform
    steps:
      - run: |
          # This FAILS - no terraform state!
          ROLE_ARN=$(terraform output -raw external_dns_role_arn)

# CORRECT - Use S3 backend
terraform {
  backend "s3" {
    bucket = "my-terraform-state"
    key    = "cdot02/terraform.tfstate"
    region = "us-west-2"
  }
}

# OR pass outputs via job outputs
jobs:
  terraform:
    outputs:
      external_dns_role_arn: ${{ steps.outputs.outputs.external_dns_role_arn }}
    steps:
      - run: terraform apply
      - id: outputs
        run: echo "external_dns_role_arn=$(terraform output -raw external_dns_role_arn)" >> $GITHUB_OUTPUT

  setup-externaldns:
    needs: terraform
    steps:
      - run: |
          ROLE_ARN="${{ needs.terraform.outputs.external_dns_role_arn }}"
```

#### 51. Nginx Permission Denied on Port 80

When nginx crashes with `bind() to 0.0.0.0:80 failed (13: Permission denied)` in a container running as non-root (uid 101):

```nginx
# WRONG - port 80 requires root
server {
    listen 80;  # Permission denied for non-root!
    ...
}

# CORRECT - use unprivileged port
server {
    listen 8080;  # Works with non-root user
    server_name localhost;
    root /usr/share/nginx/html;
    ...
}
```

Also update Kubernetes deployment and service:
```yaml
# Deployment - containerPort
ports:
  - containerPort: 8080  # Changed from 80

# Probes - port
livenessProbe:
  httpGet:
    path: /
    port: 8080  # Changed from 80

# Service - targetPort
ports:
  - name: http
    port: 80
    targetPort: 8080  # Routes external 80 to container 8080
  - name: https
    port: 443
    targetPort: 8080  # Routes external 443 to container 8080
```

#### 52. GitHub Actions Commit Loop

When workflow updates kustomization.yaml with new image tags, it triggers another workflow run, creating an infinite loop:

```yaml
# WRONG - k8s/** triggers new runs when kustomization.yaml is updated
on:
  push:
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'k8s/**'  # This causes the loop!

# CORRECT - exclude k8s folder from triggers
on:
  push:
    paths:
      - 'backend/**'
      - 'frontend/**'
      # Note: k8s/** excluded to prevent commit loops

# Also add [skip ci] to automated commits
- name: Update kustomization
  run: |
    git commit -m "[skip ci] Update image tags to ${{ env.IMAGE_TAG }}

    ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)"
```

#### 53. ArgoCD "Authentication Required" - Empty PAT

When ArgoCD shows "authentication required: Repository not found" and sync status is "Unknown":

```bash
# Check if the secret has an empty password
kubectl get secret repo-myapp-opsera -n argocd -o jsonpath='{.data.password}' | base64 -d

# If empty, the GH_PAT_TOKEN was not set during creation
# Recreate with valid token
kubectl delete secret repo-myapp-opsera -n argocd

kubectl create secret generic repo-myapp-opsera \
  --namespace argocd \
  --from-literal=url=https://github.com/org/repo.git \
  --from-literal=username=git \
  --from-literal=password=ghp_YOUR_ACTUAL_PAT_TOKEN \
  --from-literal=type=git \
  --dry-run=client -o yaml | kubectl apply -f -

kubectl label secret repo-myapp-opsera -n argocd \
  argocd.argoproj.io/secret-type=repository --overwrite

# Force ArgoCD to refresh
kubectl annotate application myapp-argo-dev -n argocd \
  argocd.argoproj.io/refresh=hard --overwrite
```

**Prevention**: Always verify PAT is set before creating secret:
```bash
if [ -z "$GH_PAT_TOKEN" ]; then
  echo "ERROR: GH_PAT_TOKEN is empty. Set it before proceeding."
  exit 1
fi
```

#### 54. ExternalDNS DNS Record Loop

When ExternalDNS logs show repeated CREATE/DELETE of the same record every sync cycle:

```
time="..." level=info msg="Desired change: DELETE old-app.domain.com A"
time="..." level=info msg="Desired change: CREATE new-app.domain.com A"
time="..." level=info msg="6 record(s) in zone domain.com were successfully updated"
# Then repeats...
```

This happens when multiple EKS clusters have ExternalDNS with the **same `txt-owner-id`**:

```yaml
# WRONG - all clusters use same owner ID
args:
  - --txt-owner-id=external-dns  # Conflict!

# CORRECT - unique owner ID per cluster/tenant
args:
  - --txt-owner-id=${TENANT}-external-dns  # e.g., cdot03-external-dns
```

The TXT owner record prevents one ExternalDNS from modifying records created by another. With the same owner ID, they fight over records.

#### 55. Manual Route53 Record Creation

When ExternalDNS has owner conflicts and can't persist records, create them manually:

```bash
# Get LoadBalancer hostname
LB_HOSTNAME=$(kubectl get svc myapp-frontend -n myapp-dev \
  -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

# NLB hosted zone IDs by region
# us-west-2: Z18D5FSROUN65G
# us-west-1: Z24FKFUX50B4VW
# us-east-1: Z26RNL4JYFTOTI
LB_ZONE_ID="Z18D5FSROUN65G"  # For us-west-2

# Create A record with alias to NLB
aws route53 change-resource-record-sets \
  --hosted-zone-id Z00814191D1XSXELJVTKT \
  --change-batch '{
    "Changes": [{
      "Action": "UPSERT",
      "ResourceRecordSet": {
        "Name": "myapp-dev.agents.opsera-labs.com",
        "Type": "A",
        "AliasTarget": {
          "HostedZoneId": "'$LB_ZONE_ID'",
          "DNSName": "'$LB_HOSTNAME'",
          "EvaluateTargetHealth": true
        }
      }
    }]
  }'

# Verify record was created
aws route53 list-resource-record-sets \
  --hosted-zone-id Z00814191D1XSXELJVTKT \
  --query "ResourceRecordSets[?Name=='myapp-dev.agents.opsera-labs.com.']"
```

#### 68. ECR Race Condition - Repos Don't Exist When GHA Runs

When GitHub Actions workflow triggers immediately after git push, but ECR repos haven't been created yet:

```
name unknown: The repository with name 'myapp-backend' does not exist in the registry with id '792373136340'
```

**Root Cause**: Git push triggers GitHub Actions workflow, but ECR repositories are created in Phase 5 (after git push in Phase 4).

**Solutions**:

1. **Create ECR repos BEFORE git push** (Recommended):
```bash
# In Phase 4, before git commit/push:
aws ecr describe-repositories --repository-names ${APP_IDENTIFIER}-backend --region ${AWS_REGION} 2>/dev/null || \
  aws ecr create-repository --repository-name ${APP_IDENTIFIER}-backend --region ${AWS_REGION}

aws ecr describe-repositories --repository-names ${APP_IDENTIFIER}-frontend --region ${AWS_REGION} 2>/dev/null || \
  aws ecr create-repository --repository-name ${APP_IDENTIFIER}-frontend --region ${AWS_REGION}

# Then git push
git push -u origin ${DEPLOYMENT_NAME}
```

2. **Re-trigger workflow after ECR creation**:
```bash
# If workflow already failed, create repos and re-trigger:
aws ecr create-repository --repository-name ${APP_IDENTIFIER}-backend --region ${AWS_REGION}
aws ecr create-repository --repository-name ${APP_IDENTIFIER}-frontend --region ${AWS_REGION}

# Re-trigger the workflow
gh workflow run "ci-cd-${APP_IDENTIFIER}.yaml" --repo ${ORG}/${REPO} --ref ${DEPLOYMENT_NAME}
```

3. **Add ECR creation step to GitHub Actions** (Self-healing):
```yaml
- name: Ensure ECR repos exist
  run: |
    aws ecr describe-repositories --repository-names ${{ env.APP_IDENTIFIER }}-backend --region ${{ env.AWS_REGION }} 2>/dev/null || \
      aws ecr create-repository --repository-name ${{ env.APP_IDENTIFIER }}-backend --region ${{ env.AWS_REGION }}
```

**Key insight**: The 5-phase deployment flow should create ECR repos in Phase 4 (before git push) rather than Phase 5 (after) to avoid this race condition.

---

## Idempotent Patterns & Restart Capability

**CRITICAL: All operations MUST be idempotent. You should be able to restart a deployment from any point without having to delete everything and start over.**

### Why Idempotency Matters

Deployments fail mid-way for many reasons:
- Network timeouts during Terraform apply
- Rate limiting on AWS API calls
- kubectl commands timing out
- CI/CD runner crashes
- Manual interruption

Without idempotent patterns, you're forced to:
1. Manually identify what was created
2. Delete everything
3. Start from scratch (painful and time-consuming!)

### Deployment State Tracking

Create a `.deployment-state.json` file to track progress and enable restart:

```bash
# Initialize deployment state at start of deployment
init_deployment_state() {
    local STATE_FILE="${DEPLOYMENT_NAME}/.deployment-state.json"
    if [ ! -f "$STATE_FILE" ]; then
        cat > "$STATE_FILE" << EOF
{
  "deployment_name": "${DEPLOYMENT_NAME}",
  "tenant": "${TENANT}",
  "app_identifier": "${APP_IDENTIFIER}",
  "environment": "${ENVIRONMENT}",
  "started_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "last_updated": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "steps": {
    "preflight_checks": "pending",
    "branch_created": "pending",
    "ecr_backend": "pending",
    "ecr_frontend": "pending",
    "terraform_init": "pending",
    "terraform_apply": "pending",
    "kubeconfig_updated": "pending",
    "namespace_created": "pending",
    "argocd_secret": "pending",
    "k8s_manifests": "pending",
    "argocd_app": "pending",
    "gha_workflow": "pending",
    "git_pushed": "pending",
    "deployment_verified": "pending"
  },
  "resources_created": {},
  "errors": []
}
EOF
    fi
    echo "$STATE_FILE"
}

# Update step status
update_step() {
    local STEP=$1
    local STATUS=$2  # pending, in_progress, completed, failed
    local STATE_FILE="${DEPLOYMENT_NAME}/.deployment-state.json"

    # Update using jq (or sed fallback)
    if command -v jq &> /dev/null; then
        jq ".steps.${STEP} = \"${STATUS}\" | .last_updated = \"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\"" \
            "$STATE_FILE" > "${STATE_FILE}.tmp" && mv "${STATE_FILE}.tmp" "$STATE_FILE"
    else
        sed -i '' "s/\"${STEP}\": \"[^\"]*\"/\"${STEP}\": \"${STATUS}\"/" "$STATE_FILE"
    fi

    echo "[$(date +%H:%M:%S)] Step: $STEP -> $STATUS"
}

# Record created resource
record_resource() {
    local RESOURCE_TYPE=$1
    local RESOURCE_ID=$2
    local STATE_FILE="${DEPLOYMENT_NAME}/.deployment-state.json"

    if command -v jq &> /dev/null; then
        jq ".resources_created.${RESOURCE_TYPE} = \"${RESOURCE_ID}\"" \
            "$STATE_FILE" > "${STATE_FILE}.tmp" && mv "${STATE_FILE}.tmp" "$STATE_FILE"
    fi
}

# Check if step is completed (for restart)
is_step_completed() {
    local STEP=$1
    local STATE_FILE="${DEPLOYMENT_NAME}/.deployment-state.json"

    if [ -f "$STATE_FILE" ]; then
        STATUS=$(jq -r ".steps.${STEP}" "$STATE_FILE" 2>/dev/null)
        [ "$STATUS" = "completed" ]
    else
        return 1
    fi
}
```

### Idempotent Resource Creation Patterns

```bash
#!/bin/bash
# All operations are idempotent - safe to run multiple times

# ============================================================================
# ECR REPOSITORY (check-before-create)
# ============================================================================
create_ecr_repo() {
    local REPO_NAME=$1
    local REGION=${2:-us-west-2}

    if aws ecr describe-repositories --repository-names "$REPO_NAME" --region "$REGION" 2>/dev/null; then
        echo "[SKIP] ECR repository exists: $REPO_NAME"
        return 0
    fi

    echo "[CREATE] ECR repository: $REPO_NAME"
    aws ecr create-repository \
        --repository-name "$REPO_NAME" \
        --region "$REGION" \
        --tags Key=tenant,Value="$TENANT" \
               Key=app-identifier,Value="$APP_IDENTIFIER" \
               Key=managed-by,Value=opsera-gitops

    record_resource "ecr_${REPO_NAME}" "$REPO_NAME"
}

# ============================================================================
# KUBERNETES NAMESPACE (declarative apply)
# ============================================================================
create_namespace() {
    local NS=$1

    kubectl create namespace "$NS" \
        --dry-run=client -o yaml | kubectl apply -f -

    # Add labels (idempotent with --overwrite)
    kubectl label namespace "$NS" \
        tenant="$TENANT" \
        app-identifier="$APP_IDENTIFIER" \
        environment="$ENVIRONMENT" \
        managed-by=opsera-gitops \
        --overwrite

    record_resource "namespace" "$NS"
}

# ============================================================================
# ARGOCD SECRET (dry-run + apply pattern)
# ============================================================================
create_argocd_secret() {
    local SECRET_NAME="repo-${DEPLOYMENT_NAME}"
    local REPO_URL=$1
    local PAT_TOKEN=$2

    kubectl create secret generic "$SECRET_NAME" \
        --namespace argocd \
        --from-literal=url="$REPO_URL" \
        --from-literal=username=git \
        --from-literal=password="$PAT_TOKEN" \
        --from-literal=type=git \
        --dry-run=client -o yaml | kubectl apply -f -

    kubectl label secret "$SECRET_NAME" -n argocd \
        argocd.argoproj.io/secret-type=repository \
        --overwrite

    record_resource "argocd_secret" "$SECRET_NAME"
}

# ============================================================================
# TERRAFORM (import existing + plan before apply)
# ============================================================================
terraform_idempotent_apply() {
    local TF_DIR=$1
    cd "$TF_DIR"

    # Initialize (idempotent)
    terraform init -upgrade

    # Import existing IAM roles if they exist
    for ROLE_SUFFIX in external-dns eks-node eks-cluster; do
        ROLE_NAME="${TENANT}-${ROLE_SUFFIX}"
        RESOURCE="aws_iam_role.${ROLE_SUFFIX//-/_}"

        # Check if already in state
        if terraform state show "$RESOURCE" 2>/dev/null; then
            echo "[SKIP] $RESOURCE already in Terraform state"
            continue
        fi

        # Check if role exists in AWS
        if aws iam get-role --role-name "$ROLE_NAME" 2>/dev/null; then
            echo "[IMPORT] Importing existing role: $ROLE_NAME"
            terraform import "$RESOURCE" "$ROLE_NAME" || true
        fi
    done

    # Import existing VPC if tagged
    EXISTING_VPC=$(aws ec2 describe-vpcs \
        --filters "Name=tag:tenant,Values=$TENANT" \
        --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
    if [ "$EXISTING_VPC" != "None" ] && [ -n "$EXISTING_VPC" ]; then
        if ! terraform state show "module.vpc.aws_vpc.this[0]" 2>/dev/null; then
            echo "[IMPORT] Importing existing VPC: $EXISTING_VPC"
            terraform import "module.vpc.aws_vpc.this[0]" "$EXISTING_VPC" || true
        fi
    fi

    # Plan and apply
    terraform plan -out=tfplan
    terraform apply tfplan

    cd -
}

# ============================================================================
# ARGOCD APPLICATION (server-side apply)
# ============================================================================
apply_argocd_app() {
    local APP_YAML=$1

    # Use server-side apply for better idempotency
    kubectl apply -f "$APP_YAML" --server-side --force-conflicts

    record_resource "argocd_app" "$(basename "$APP_YAML" .yaml)"
}

# ============================================================================
# GIT OPERATIONS (force with lease for safety)
# ============================================================================
git_push_idempotent() {
    # Pull latest changes first (handles CI auto-commits)
    git pull --rebase origin "$DEPLOYMENT_NAME" || true

    # Stage all changes
    git add -A

    # Check if there are changes to commit
    if git diff --cached --quiet; then
        echo "[SKIP] No changes to commit"
        return 0
    fi

    # Commit
    git commit -m "Update deployment configs for $DEPLOYMENT_NAME

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"

    # Push (with lease for safety)
    git push --force-with-lease origin "$DEPLOYMENT_NAME"
}
```

### Restart From Any Point

```bash
#!/bin/bash
# Resume deployment from last successful step

resume_deployment() {
    local STATE_FILE="${DEPLOYMENT_NAME}/.deployment-state.json"

    if [ ! -f "$STATE_FILE" ]; then
        echo "No deployment state found. Starting fresh deployment."
        return 1
    fi

    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  RESUMING DEPLOYMENT                                                          â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Deployment: $DEPLOYMENT_NAME"
    echo "Last updated: $(jq -r '.last_updated' "$STATE_FILE")"
    echo ""
    echo "Step Status:"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

    # Show step status
    for STEP in preflight_checks branch_created ecr_backend ecr_frontend \
                terraform_init terraform_apply kubeconfig_updated namespace_created \
                argocd_secret k8s_manifests argocd_app gha_workflow git_pushed deployment_verified; do
        STATUS=$(jq -r ".steps.${STEP}" "$STATE_FILE")
        case $STATUS in
            completed) echo "  âœ… $STEP" ;;
            in_progress) echo "  ğŸ”„ $STEP (will resume)" ;;
            failed) echo "  âŒ $STEP (will retry)" ;;
            pending) echo "  â¬œ $STEP" ;;
        esac
    done

    echo ""
    echo "Resuming from first incomplete step..."
    echo ""

    # Resume from first non-completed step
    for STEP in preflight_checks branch_created ecr_backend ecr_frontend \
                terraform_init terraform_apply kubeconfig_updated namespace_created \
                argocd_secret k8s_manifests argocd_app gha_workflow git_pushed deployment_verified; do

        if is_step_completed "$STEP"; then
            echo "[SKIP] $STEP already completed"
            continue
        fi

        # Execute step
        update_step "$STEP" "in_progress"

        case $STEP in
            preflight_checks)
                ./preflight-checks.sh "$AWS_REGION" "$TENANT" && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            ecr_backend)
                create_ecr_repo "${APP_IDENTIFIER}-backend" us-west-2 && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            ecr_frontend)
                create_ecr_repo "${APP_IDENTIFIER}-frontend" us-west-2 && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            terraform_apply)
                terraform_idempotent_apply "${DEPLOYMENT_NAME}/terraform" && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            namespace_created)
                create_namespace "${APP_IDENTIFIER}-${ENVIRONMENT}" && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            argocd_secret)
                create_argocd_secret "$REPO_URL" "$PAT_TOKEN" && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            argocd_app)
                apply_argocd_app "${DEPLOYMENT_NAME}/argocd/application.yaml" && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            git_pushed)
                git_push_idempotent && update_step "$STEP" "completed" || update_step "$STEP" "failed"
                ;;
            *)
                echo "[INFO] Step $STEP requires manual execution"
                update_step "$STEP" "completed"
                ;;
        esac

        # Check if step failed
        if [ "$(jq -r ".steps.${STEP}" "$STATE_FILE")" = "failed" ]; then
            echo ""
            echo "âŒ Step $STEP failed. Fix the issue and run resume_deployment again."
            return 1
        fi
    done

    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  âœ… DEPLOYMENT RESUMED SUCCESSFULLY                                           â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
}
```

### Quick Reference: Idempotent Commands

| Operation | Idempotent Command | Notes |
|-----------|-------------------|-------|
| Create namespace | `kubectl create ns X --dry-run=client -o yaml \| kubectl apply -f -` | Always safe |
| Create secret | `kubectl create secret ... --dry-run=client -o yaml \| kubectl apply -f -` | Always safe |
| Apply manifest | `kubectl apply -f X.yaml --server-side` | Server-side is safer |
| ECR repo | Check with `describe-repositories` first | AWS will error on duplicate |
| IAM role | Import to Terraform state if exists | Avoids "already exists" error |
| ArgoCD app | `kubectl apply --server-side --force-conflicts` | Handles field ownership |
| Git push | `git push --force-with-lease` | Safe force push |
| Terraform | `terraform import` before `apply` | Import existing resources |

### Recovery Commands

```bash
# Check deployment state
cat ${DEPLOYMENT_NAME}/.deployment-state.json | jq .

# Reset a failed step to pending (to retry)
jq '.steps.terraform_apply = "pending"' .deployment-state.json > tmp.json && mv tmp.json .deployment-state.json

# Mark all steps as pending (full restart without deleting resources)
jq '.steps |= with_entries(.value = "pending")' .deployment-state.json > tmp.json && mv tmp.json .deployment-state.json

# Resume deployment
source ./deployment-functions.sh && resume_deployment
```

---

## Pod Capacity Reference

| Instance Type | Max Pods | Use Case |
|---------------|----------|----------|
| t3.small | 11 | Testing only |
| t3.medium | 17 | Dev environments |
| t3.large | 35 | Small production |
| t3.xlarge | 58 | Medium production |
| m5.large | 29 | Production |
| m5.xlarge | 58 | Large production |

---

## Troubleshooting Quick Reference

### Diagnostic Commands

```bash
# Check non-running pods
kubectl get pods -A | grep -v Running

# Describe failing pod
kubectl describe pod <pod> -n <namespace> | tail -30

# Get pod logs
kubectl logs <pod> -n <namespace> --previous --tail=50

# Check ArgoCD status
kubectl get application <app> -n argocd -o jsonpath='{.status.sync.status} / {.status.health.status}'

# Check ExternalDNS logs
kubectl logs -l app=external-dns -n kube-system --tail=50

# Check node capacity
kubectl get nodes -o custom-columns=NAME:.metadata.name,PODS:.status.capacity.pods

# Force ArgoCD refresh
kubectl annotate application <app> -n argocd argocd.argoproj.io/refresh=hard --overwrite
```

### Common Issues

| Status | Issue | Fix |
|--------|-------|-----|
| `ImagePullBackOff` | Tag mismatch | Check ECR for correct tag format (short SHA) |
| `CrashLoopBackOff` | Missing volumes | Add emptyDir for nginx, tmp |
| `CrashLoopBackOff` | Nginx upstream error | Check nginx.conf backend service name |
| `Pending` | Pod capacity | Use larger instance or scale cluster |
| `OutOfSync` | Manifest drift | Force refresh ArgoCD |
| `Unknown` | Repo access | Check ArgoCD repo secret |
| `SyncLoadBalancerFailed` | Cert not found | Wait for ACM ISSUED status |
| `SyncLoadBalancerFailed` | Wrong account cert | Create cert in correct account |
| LoadBalancer `<pending>` | NLB provisioning | Wait 2-5 minutes |
| DNS not resolving | Propagation delay | Test via `@8.8.8.8`, flush local cache |
| HTTPS connection refused | Cert not ready | Check ACM status, recreate service |

### DNS Troubleshooting

```bash
# Check nameserver propagation
dig yourdomain.com NS +short

# Check record via Google DNS (bypasses local cache)
dig myapp.yourdomain.com @8.8.8.8 +short

# Flush local DNS cache
# Mac:
sudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder
# Windows:
ipconfig /flushdns
# Linux:
sudo systemd-resolve --flush-caches
```

### ACM Certificate Troubleshooting

```bash
# Check certificate status
aws acm describe-certificate --certificate-arn <ARN> --query 'Certificate.Status'

# Check validation status
aws acm describe-certificate --certificate-arn <ARN> \
  --query 'Certificate.DomainValidationOptions[*].[DomainName,ValidationStatus]'

# List all certificates
aws acm list-certificates --query 'CertificateSummaryList[*].[DomainName,Status]'
```

### LoadBalancer Troubleshooting

```bash
# Check service events
kubectl describe svc myapp-frontend -n myapp | tail -20

# Check NLB status
aws elbv2 describe-load-balancers --query 'LoadBalancers[*].[LoadBalancerName,State.Code]'

# Check NLB listeners (verify HTTPS listener exists)
LB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[0].LoadBalancerArn' --output text)
aws elbv2 describe-listeners --load-balancer-arn $LB_ARN

# Force service recreation
kubectl delete svc myapp-frontend -n myapp
kubectl apply -f k8s/base/frontend-service.yaml -n myapp
```

---

## Environment Matrix

| Config | Dev | Staging | Prod |
|--------|-----|---------|------|
| **Replicas** | 1 | 2 | 3+ |
| **Service Type** | LoadBalancer | ClusterIP | ClusterIP |
| **Memory Requests** | 64Mi | 128Mi | 256Mi |
| **Memory Limits** | 128Mi | 256Mi | 512Mi |
| **ArgoCD Sync** | Auto | Auto | Manual |
| **PodDisruptionBudget** | No | No | Yes |
| **Instance Type** | t3.medium | t3.large | m5.xlarge |

---

## Supported Languages

| Language | Base Image | Build | Port |
|----------|------------|-------|------|
| **Python** | python:3.12-slim | pip + gunicorn | 8000 |
| **Node.js** | node:20-alpine | npm | 3000 |
| **Java** | eclipse-temurin:17 | maven/gradle | 8080 |
| **Go** | golang:1.22-alpine | go build | 8080 |
| **Frontend** | nginx:alpine | npm build | 3000 (nginx) |

---

## Cleanup Procedure

**Order matters! Delete ArgoCD app BEFORE namespace:**

```bash
# Use values from your DEPLOYMENT-CONTEXT.md
APP_IDENTIFIER="<identifier>"     # e.g., myapp
ENVIRONMENT="<env>"               # e.g., dev
DEPLOYMENT_NAME="${APP_IDENTIFIER}-${ENVIRONMENT}"  # e.g., myapp-dev
ARGO_APP_NAME="${APP_IDENTIFIER}-argo-${ENVIRONMENT}"  # e.g., myapp-argo-dev

# 1. Delete ArgoCD application FIRST (uses argo naming)
kubectl delete application $ARGO_APP_NAME -n argocd

# 2. Wait for namespace deletion
kubectl delete namespace $DEPLOYMENT_NAME

# 3. Delete ArgoCD repo secret
kubectl delete secret repo-$DEPLOYMENT_NAME -n argocd

# 4. (Optional) Delete ECR repos (shared across environments)
aws ecr delete-repository --repository-name $APP_IDENTIFIER-backend --force
aws ecr delete-repository --repository-name $APP_IDENTIFIER-frontend --force

# 5. (Optional) Delete local branch and folder
git checkout main
git branch -D $DEPLOYMENT_NAME
rm -rf $DEPLOYMENT_NAME/

# 6. Verify cleanup by tags
echo "Remaining resources with app-identifier=$APP_IDENTIFIER:"
kubectl get all -A -l app-identifier=$APP_IDENTIFIER
```

---

## Verified Working Endpoints

| App | Type | URL | Date | Notes |
|-----|------|-----|------|-------|
| **cdot8-home** | React + Python | https://cdot8-home-dev.agents.opsera-labs.com | **Jan 2026** | Brownfield-hub-spoke, ECR race condition fix, full GHA CI/CD |
| cdot03-home | React + Python | https://cdot03-home-dev.agents.opsera-labs.com | Jan 2026 | Greenfield, nginx port fix, ExternalDNS owner conflict, manual DNS |
| cdot02-home | React + Python | https://cdot02-home-dev.agents.opsera-labs.com | Jan 2026 | Greenfield, ACM region fix, IRSA multi-cluster |
| cdot01-home | React + Python | https://cdot01-home-dev.agents.opsera-labs.com | Jan 2026 | Greenfield, ExternalDNS fixes |
| awaaz01-dev | React + Python | https://awaaz01-dev.agents.opsera-labs.com | Jan 2026 | Brownfield-embedded |
| awaaz05 | React + Python | https://awaaz05.selfcaretech.com | Jan 2026 | Custom domain (GoDaddy) |
| presales-pipeline | React + Python | https://presales-pipeline.agents.opsera-labs.com | Dec 2025 | Production verified |
| prospect-context | Python | https://prospect-context.agents.opsera-labs.com | Dec 2025 | Backend only |

---

## Multi-Account Deployment Checklist

When deploying to a **new AWS account**:

- [ ] Create VPC with public/private subnets (Terraform)
- [ ] Create EKS cluster with IRSA enabled (Terraform)
- [ ] Create ECR repositories (AWS CLI)
- [ ] Install ArgoCD (kubectl)
- [ ] Install ExternalDNS with IRSA role (kubectl)
- [ ] Create Route53 hosted zone for custom domain
- [ ] Update domain registrar nameservers to Route53
- [ ] Request ACM wildcard certificate
- [ ] Create DNS validation record in Route53
- [ ] Wait for certificate to be ISSUED
- [ ] Update frontend service with new cert ARN
- [ ] Create ArgoCD repo secret for private repo
- [ ] Create ArgoCD Application
- [ ] Push code to trigger CI/CD
- [ ] Verify LoadBalancer becomes active
- [ ] Update Route53 record with LoadBalancer hostname
- [ ] Verify HTTPS endpoint

---

## Skills This Replaces

This unified skill consolidates:
- `container-eks-gitops`
- `gitops-eks-deploy`
- `kustomize-app-deploy`
- `ecr-image-push`
- `route53-friendly-url`
- `opsera_cicd_container_aws`
- `terraform-eks-patterns`
- `idempotent-scripts`
- `k8s-debugging-guide`

---

## Handoff Document Template

When handing off a deployment to another team member, create a `DEPLOYMENT-STATUS.md` with:

```markdown
# Deployment Handoff

## AWS Account
- Account ID:
- ECR Region:
- EKS Region:
- Cluster Name:

## Credentials
- AWS_ACCESS_KEY_ID=
- AWS_SECRET_ACCESS_KEY=

## Domain
- Domain:
- Route53 Zone ID:
- ACM Certificate ARN:
- Certificate Status:

## Current LoadBalancer
- Hostname:
- HTTP Port: 80
- HTTPS Port: 443
- Status:

## Quick Connect
aws eks update-kubeconfig --name <cluster> --region <region>
kubectl get pods -n <namespace>
kubectl get svc -n <namespace>

## Known Issues
- [List any pending issues]

## Next Steps
- [What needs to be done]
```

---

## Changelog & Improvements

### Version History

| Date | Version | Changes |
|------|---------|---------|
| 2026-01-05 | 3.6.0 | **cdot8-home Learnings + Brownfield-Hub-Spoke Validation**: ECR race condition fix (#68) - repos must be created before git push to avoid GHA failure. Brownfield-hub-spoke pattern fully validated with argocd-nonprod-eks â†’ customer0-nonprod-eks. GitHub Actions CI/CD pipeline tested end-to-end. Minimal human inputs validated (only 4 interactions for complete deployment). ~10 minute deployment time achieved. |
| 2026-01-04 | 3.5.0 | **Standardized 5-Phase Deployment Flow**: Enforced consistent flow across all deployments: (1) Collect Information, (2) Generate Plan & User Confirmation, (3) Prerequisite Validation (GHA, AWS secrets, tools), (4) Check-in to Git, (5) Execute Infraâ†’Platformâ†’App. Added prerequisite validation script, deployment plan template, and flow enforcement rules. |
| 2026-01-04 | 3.4.0 | **mario1-app1 & test-green Learnings + GitOps Guardrails**: Cross-account fixes (#56-57), Kubernetes fixes (#58-59), CI/CD fixes (#60-63), Critical guardrails (#64-67) for GitOps enforcement, multi-account pre-planning, preview-before-operating, and generic AI fallback detection. 12 new fixes from Gayathri & Manasa's E2E testing. |
| 2026-01-04 | 3.3.0 | **cdot03-home Learnings + Non-Root Container Fixes**: Nginx port 80 permission denied fix (#51), GitHub Actions commit loop prevention (#52), ArgoCD empty PAT detection (#53), ExternalDNS owner ID conflict resolution (#54), manual Route53 record creation (#55), cdot03-home verified deployment |
| 2026-01-04 | 3.2.0 | **Greenfield Pattern Hardening + cdot02 Learnings**: ACM region mismatch fix, cross-cluster registration timeout workaround, IRSA multi-cluster trust policy, VPC cleanup order, Terraform S3 backend recommendation, 7 new verified fixes (#44-50), cdot02-home verified deployment |
| 2026-01-04 | 3.1.0 | **Reliability & Restart Capability**: AWS capacity pre-flight checks, idempotent operations with state tracking, GHA workflow location fix, resume from any point capability, 3 new verified fixes (#41-43) |
| 2026-01-04 | 3.0.0 | **Touchless Deployment + Opsera Workbench**: Auto-generated local monitoring dashboard, ExternalDNS RBAC/domain-filter fixes, EKS admin permissions fix, HTTPS by default, 4 new verified fixes (#37-40), cdot01-home verified deployment |
| 2026-01-04 | 2.5.0 | **ArgoCD vs Target Cluster Clarity + ApplicationSet**: Clear distinction between ArgoCD cluster (management) and Target cluster (workloads), added ApplicationSet templates for multi-env, updated Greenfield to create proper target cluster |
| 2026-01-04 | 2.4.0 | **Folder Structure Cleanup**: Flattened `gitops/argocd/` to `argocd/`, changed to `{app}-opsera` naming, separated deployment configs from app source code |
| 2026-01-04 | 2.3.0 | **Opsera Branding + awaaz01-dev Learnings**: Added Opsera ASCII banners, 6 new verified fixes (#31-36), pre-deployment checklist, expanded test coverage |
| 2026-01-04 | 2.2.0 | Added TENANT as 5th input, separated tenant-level vs app-level resources |
| 2026-01-04 | 2.1.0 | Added GitOps architecture patterns (greenfield, brownfield-embedded, brownfield-hub-spoke, brownfield-cluster) |
| 2026-01-04 | 2.0.0 | Major update: Added mandatory inputs, environment-aware naming, resource tagging |
| 2026-01-03 | 1.0.0 | Initial unified skill consolidating 9 separate skills |

---

### Improvements Made (2026-01-04)

#### 1. Mandatory Input Collection
**Problem**: Skill allowed proceeding without structured inputs, leading to inconsistent naming
**Solution**: Added "âš ï¸ MANDATORY FIRST STEP" section that:
- Prompts for `APP_IDENTIFIER`, `AWS_REGION`, and `ENVIRONMENT` before any action
- Provides exact prompt template for collecting inputs
- Blocks progress until inputs collected

#### 2. Environment-Aware Naming Convention
**Problem**: No environment distinction in resource names
**Solution**: All resources now use `{identifier}-{env}` pattern:
- Branch: `{identifier}-{env}` (e.g., `myapp-dev`)
- Namespace: `{identifier}-{env}` (e.g., `myapp-dev`)
- ArgoCD App: `{identifier}-argo-{env}` (e.g., `myapp-argo-dev`)
- DNS: `{identifier}-{env}.agents.opsera-labs.com`

#### 3. Early Git Commit Workflow
**Problem**: Work could be lost if not committed early
**Solution**: Step 0.2 now includes immediate git operations:
- Create branch
- Create folder structure
- Create DEPLOYMENT-CONTEXT.md
- Initial commit
- Push to remote

#### 4. Deployment Context Tracking
**Problem**: No central place to track deployment configuration
**Solution**: Auto-generated `DEPLOYMENT-CONTEXT.md` in each deployment folder:
- Tracks identifiers, AWS config, endpoints
- Includes resource tags
- Includes status checklist
- Created timestamp

#### 5. Resource Tagging Standard
**Problem**: Cloud resources not tagged, hard to track/audit
**Solution**: Added mandatory tagging section with 5 standard tags:
- `app-identifier`: Links all resources for an app
- `environment`: dev/staging/prod
- `deployment-name`: Full deployment reference
- `managed-by`: opsera-gitops
- `created-by`: claude-code

Includes examples for AWS CLI, Kubernetes labels, and ArgoCD manifests.

#### 6. Agent Behavior Rules
**Problem**: No guidance on how Claude Code should use this skill
**Solution**: Added "ğŸ¤– AGENT BEHAVIOR RULES" section that:
- Establishes this skill as single source of truth
- Prevents mixing with other deployment skills
- Requires gap documentation for missing features

#### 7. Self-Contained Skill
**Problem**: Skill depended on external files (CLAUDE.md, SKILL-FEEDBACK.md)
**Solution**: Incorporated all guidance directly into the skill file

#### 8. GitOps Architecture Patterns (v2.1.0)
**Problem**: No guidance on different infrastructure scenarios (new vs existing clusters)
**Solution**: Added 4 architecture patterns with visual diagrams:
- **Greenfield**: Create NEW ArgoCD cluster + NEW workload cluster
- **Brownfield-Embedded**: Use EXISTING cluster with ArgoCD already installed
- **Brownfield-Hub-Spoke**: Use EXISTING ArgoCD hub + EXISTING workload cluster
- **Brownfield-Cluster**: Use EXISTING cluster + create NEW ArgoCD

Includes:
- Visual ASCII diagrams for each pattern
- Comparison matrix (complexity, time, cost, use cases)
- Pattern-specific setup guides with step-by-step commands
- Pattern selection as 4th mandatory input
- Pattern-specific checklists in DEPLOYMENT-CONTEXT.md

#### 9. Tenant vs App Resource Scoping (v2.2.0)
**Problem**: Skill generated VPC/cluster per app instead of per tenant
**Solution**: Added TENANT as 5th required input with clear resource scoping:
- **Tenant-level** (shared): VPC, ArgoCD cluster, workload cluster
- **App-level** (per-app): ECR repos, namespace, ArgoCD app

Includes:
- Resource scoping diagram
- Clear separation of Terraform (app-level ECR only)
- Tenant tag added to all resources
- Updated prompt template for 5 inputs

#### 10. Opsera Branding & Professional Output (v2.3.0)
**Problem**: Skill lacked professional branding and visual identity
**Solution**: Added Opsera branding throughout the skill:
- ASCII art Opsera logo in header
- "Powered by Opsera" taglines
- Branded pre-deployment checklist box
- Deployment completion banner template

#### 11. Pre-Deployment Verification Checklist (v2.3.0)
**Problem**: Deployments failed due to missing prerequisites (certs, secrets, capacity)
**Solution**: Added comprehensive pre-deployment checklist covering:
- Infrastructure (AWS, kubectl, ArgoCD, ExternalDNS)
- Certificates & DNS (ACM status, Route53, propagation)
- Code & Configuration (/health, CORS, nginx.conf, Dockerfiles)
- Git & ArgoCD (branch, secrets, URLs, .gitignore)
- Kubernetes (namespace, capacity, stuck pods, annotations)

Plus quick verification commands for each category.

#### 12. Six New Verified Fixes from awaaz01-dev (v2.3.0)
**Problem**: New edge cases discovered during awaaz01-dev deployment
**Solution**: Added fixes #31-36:
- #31: ArgoCD SSH_AUTH_SOCK error (SSH vs HTTPS URL mismatch)
- #32: Backend CrashLoopBackOff (missing /health endpoint)
- #33: Frontend 502 Bad Gateway (CORS not configured)
- #34: Terraform files committed (missing .gitignore)
- #35: ACM certificate not found (wrong ARN verification)
- #36: Stuck Terminating pods (force delete command)

Each fix includes detailed explanation, code examples, and resolution steps.

#### 13. Folder Structure Simplification (v2.4.0)
**Problem**: Nested `gitops/argocd/` folder was unnecessarily deep, app code mixed with deployment configs
**Solution**:
- Flattened `gitops/argocd/` to just `argocd/`
- Removed `backend/`, `frontend/`, `scripts/` from deployment folder
- Deployment folder now contains ONLY deployment configs

Structure:
```
{app}-opsera/           # Deployment configs only
â”œâ”€â”€ argocd/
â”œâ”€â”€ terraform/
â”œâ”€â”€ k8s/
â””â”€â”€ .github/workflows/
```

#### 14. App-Based Branch/Folder Naming (v2.4.0)
**Problem**: Branch/folder naming was inconsistent
**Solution**: Changed to `{app}-opsera` pattern:
- Old: `awaaz01-dev/` (mixed app code and deployment)
- New: `myapp-opsera/` (deployment configs only)

Benefits:
- Clear separation of deployment configs from app source code
- App source code lives at repo root (or separate repo)
- Cleaner, simpler deployment folder structure
- `opsera` suffix clearly identifies deployment configs

#### 15. Opsera Workbench - Local Monitoring Dashboard (v3.0.0)
**Problem**: No local visibility into GitOps deployment progress
**Solution**: Auto-generate `.opsera/deployment-monitor/` (Opsera Workbench) during bootstrap:
- Real-time GitHub Actions workflow monitoring
- Visual pipeline topology with stage status
- Endpoint health checking
- Dockerized for easy startup: `docker-compose up`
- Config-driven: edit `config.json` to customize

See "Opsera Workbench Auto-Generation" section below.

#### 16. ExternalDNS Complete Configuration (v3.0.0)
**Problem**: ExternalDNS deployments failed due to missing RBAC, wrong domain-filter, or empty IRSA role
**Solution**: Added complete ExternalDNS setup with:
- ClusterRole and ClusterRoleBinding for API access (#37)
- Parent zone domain-filter (opsera-labs.com not agents.opsera-labs.com) (#38)
- IRSA role with fallback from Terraform output (#39)
- All three components required for ExternalDNS to work

#### 17. EKS Cluster Creator Admin Permissions (v3.0.0)
**Problem**: GitHub Actions IAM user couldn't access EKS cluster after Terraform creation
**Solution**: Add `enable_cluster_creator_admin_permissions = true` to EKS module (#40)

#### 18. HTTPS by Default (v3.0.0)
**Problem**: Services created with HTTP only, requiring manual HTTPS fix
**Solution**: All frontend service templates now include:
- ACM certificate annotation for NLB SSL termination
- Port 443 for HTTPS
- Port 80 for HTTP redirect
- Configured in base manifests, not overlays

#### 23. cdot03-home Learnings (v3.3.0)
**Problem**: Non-root containers can't bind to privileged ports; CI/CD loops; ArgoCD auth failures; ExternalDNS conflicts
**Solution**: Added 5 new verified fixes from cdot03-home deployment:
- Fix #51: Nginx port 80 permission denied - use port 8080 for non-root containers
- Fix #52: GitHub Actions commit loop - exclude k8s/** from triggers, use [skip ci]
- Fix #53: ArgoCD empty PAT - verify PAT before creating secret, recreate with valid token
- Fix #54: ExternalDNS owner conflict - use unique txt-owner-id per cluster
- Fix #55: Manual Route53 creation - create A record with alias when ExternalDNS can't persist

Key insights:
- Non-root containers (uid 101 for nginx) cannot bind to ports < 1024
- Multiple EKS clusters with same ExternalDNS txt-owner-id will fight over records
- ArgoCD repo secrets with empty passwords fail silently
- NLB hosted zone IDs vary by region (us-west-2: Z18D5FSROUN65G)

#### 25. cdot8-home Learnings (v3.6.0)
**Problem**: ECR race condition, brownfield-hub-spoke validation needed, GHA CI/CD untested
**Solution**: Added Fix #68 and validated the brownfield-hub-spoke pattern:

**ECR Race Condition (Fix #68)**:
- GitHub Actions triggered before ECR repos were created
- Error: "repository does not exist in the registry"
- Solution: Create ECR repos BEFORE git push, or re-trigger workflow after creation

**Brownfield-Hub-Spoke Pattern Validated**:
- ArgoCD cluster: `argocd-nonprod-eks`
- Workload cluster: `customer0-nonprod-eks` (registered as `customer0-cluster`)
- ArgoCD Application targets workload cluster via registered cluster URL
- Pattern works smoothly when cluster is already registered

**Deployment Metrics**:
- Total deployment time: ~10 minutes
- Human interactions required: 4 (5 params, cluster names, PAT, approval)
- Automatic code fixes applied: 3 (nginx port, CORS, /health endpoint)
- Files generated: 16

Key insights:
- ECR repos should be created in Phase 4 (before git push) to avoid race condition
- Brownfield-hub-spoke is ideal when ArgoCD and workload clusters already exist
- Full GitOps automation achievable with minimal human input
- Pre-registered clusters in ArgoCD simplify the deployment flow

#### 24. mario1-app1 & test-green Learnings (v3.4.0)
**Problem**: Cross-account confusion, kubectl bypass of GitOps, missing previews, generic AI fallback
**Solution**: Added 12 new verified fixes from Gayathri & Manasa's testing:

**Multi-Account Fixes:**
- Fix #56: ACM/Route53 Cross-Account Limitation - ACM certs and Route53 zones are account-specific; cannot use cross-account resources
- Fix #57: Wrong AWS Account ID in ECR URL - ImagePullBackOff when account ID mismatch; always verify with `aws sts get-caller-identity`

**Kubernetes Fixes:**
- Fix #58: Deployment Selector Immutability - Cannot change selector on existing deployment; use `kubectl set image` or delete/recreate
- Fix #59: Classic LoadBalancer Fallback - NLB/ALB require AWS LB Controller; Classic LB works without extra setup

**CI/CD Fixes:**
- Fix #60: Chrome HTTP Blocking - Chrome blocks HTTP; use Brave/Firefox or chrome://flags for testing
- Fix #61: GitHub Actions Workflow Location - Workflows MUST be at `.github/workflows/` (repo root); workflows inside folders IGNORED
- Fix #62: Docker Platform Mismatch (ARM64 vs AMD64) - Apple Silicon builds ARM64; always use `--platform linux/amd64` for EKS
- Fix #63: Git Conflicts from CI/CD Auto-Commits - CI/CD updates cause conflicts; always `git pull --rebase` before pushing

**Critical Guardrails Added:**
- Guardrail #64: GitOps Violation Detection - Pause before kubectl mutating commands, offer Git-based alternative
- Guardrail #65: Multi-Account Pre-Planning - Verify all resources in same account BEFORE deployment
- Guardrail #66: Preview Before Operating - Always preview (terraform plan, dry-run) before applying
- Guardrail #67: Generic AI Fallback Detection - Detect and call out when skill can't help, document gaps

Key insights:
- Account isolation is critical: ACM, Route53, ECR are all account-specific
- Classic LB is the safe default without AWS Load Balancer Controller
- GitHub Actions workflows only work at repo root
- Always prefer GitOps flow over manual kubectl
- Preview before operating should be default behavior

#### 19. Greenfield Pattern Hardening (v3.2.0)
**Problem**: Greenfield pattern failed with private subnets due to cross-cluster communication timeout
**Solution**: Added comprehensive documentation and fixes:
- Fix #44: ACM certificate region mismatch detection and resolution
- Fix #45: Cross-cluster registration timeout workaround (VPC peering or in-cluster fallback)
- Fix #46: IRSA multi-cluster trust policy for ExternalDNS
- Warning added to Greenfield pattern documentation

#### 20. VPC Cleanup Script (v3.2.0)
**Problem**: VPC deletion failed due to circular dependencies between resources
**Solution**: Added comprehensive VPC cleanup script (Fix #49):
- Ordered deletion: NAT GW â†’ SG rules â†’ SGs â†’ Subnets â†’ IGW â†’ RTBs â†’ VPC
- Security group rule revocation to break circular dependencies
- Waiter for NAT Gateway deletion
- Idempotent operations with error handling

#### 21. Terraform State Sharing (v3.2.0)
**Problem**: Terraform outputs not available between GitHub Actions jobs
**Solution**: Documented two approaches (Fix #50):
- Use S3 backend for shared state (recommended)
- Pass outputs via GitHub Actions job outputs

#### 22. ACM Certificate Region Awareness (v3.2.0)
**Problem**: Deployments failed when ACM cert was in wrong region
**Solution**: Added Fix #44:
- Clear documentation that ACM certs are region-specific
- Commands to check/create certs in correct region
- Key insight: DNS validation reuses existing CNAME records

---

### Gaps Identified

| # | Date | Scenario | What Was Missing | Severity | Status |
|---|------|----------|------------------|----------|--------|
|   |      |          |                  |          |        |

*Add gaps here as they are discovered during usage.*

---

### Test Scenarios

Track skill testing coverage:

| Scenario | Status | Notes |
|----------|--------|-------|
| Initial skill invocation | âœ… Tested | Multiple deployments (Jan 2026) |
| ECR setup guidance | âœ… Tested | Backend/frontend repos created |
| EKS deployment | âœ… Tested | Multiple clusters, various regions |
| ArgoCD configuration | âœ… Tested | HTTPS repo secret, SSHâ†’HTTPS fix applied |
| Kustomize overlays | âœ… Tested | base + dev overlay with image tags |
| GitHub Actions CI/CD | âœ… Tested | cdot8-home: Full GHA pipeline with ECR push, kustomize update |
| Route53/DNS setup | âœ… Tested | ExternalDNS auto-created A record |
| SSL/HTTPS configuration | âœ… Tested | NLB with ACM cert annotations |
| Multi-environment deployment | â¬œ Not tested | Only dev tested so far |
| Rollback procedures | â¬œ Not tested | |
| New AWS account setup | âœ… Tested | Fresh AWS account with custom domain |
| Custom domain (GoDaddy) setup | âœ… Tested | awaaz05.selfcaretech.com |
| Cleanup procedure | â¬œ Not tested | |
| Pod capacity planning | âœ… Tested | Hit pod limits, force-deleted old pods |
| Backend /health endpoint | âœ… Tested | Fix #32 - required health endpoint |
| CORS production config | âœ… Tested | Fix #33 - production URL in CORS |
| Terraform .gitignore | âœ… Tested | Fix #34 - exclude .terraform/ |
| ACM cert verification | âœ… Tested | Fix #35 - verify cert ARN |
| ExternalDNS RBAC setup | âœ… Tested | Fix #37 - ClusterRole/ClusterRoleBinding |
| ExternalDNS domain-filter | âœ… Tested | Fix #38 - use parent zone |
| ExternalDNS IRSA role | âœ… Tested | Fix #39 - fallback ARN |
| EKS admin permissions | âœ… Tested | Fix #40 - enable_cluster_creator_admin_permissions |
| Greenfield pattern | âœ… Tested | New VPC, EKS, ECR |
| Opsera Workbench | âœ… Tested | Local dashboard at :8888 |
| ACM cert region mismatch | âœ… Tested | Fix #44 - same region for all |
| Cross-cluster ArgoCD timeout | âœ… Tested | Fix #45 - private subnet workaround |
| IRSA multi-cluster trust | âœ… Tested | Fix #46 - multi OIDC trust |
| ACM DNS validation reuse | âœ… Tested | Fix #47 - instant validation |
| LoadBalancer cert update | âœ… Tested | Fix #48 - delete/recreate |
| VPC cleanup with dependencies | âœ… Tested | Fix #49 - ordered deletion |
| Terraform outputs in GHA | âœ… Tested | Fix #50 - S3 backend |
| Greenfield private subnets | âœ… Tested | In-cluster fallback |
| Nginx non-root port binding | âœ… Tested | Fix #51: cdot03 port 80â†’8080 for uid 101 |
| GHA commit loop prevention | âœ… Tested | Fix #52: cdot03 exclude k8s/**, [skip ci] |
| ArgoCD empty PAT detection | âœ… Tested | Fix #53: cdot03 secret recreation with valid token |
| ExternalDNS owner conflict | âœ… Tested | Fix #54: cdot03 unique txt-owner-id per cluster |
| Manual Route53 DNS creation | âœ… Tested | Fix #55: cdot03 alias A record to NLB |
| Cross-account ACM/Route53 | âœ… Tested | Fix #56: mario1-app1 account-specific resources |
| Wrong account ID in ECR URL | âœ… Tested | Fix #57: mario1-app1 ImagePullBackOff fix |
| Deployment selector immutability | âœ… Tested | Fix #58: mario1-app1 kubectl set image workaround |
| Classic LB fallback (no AWS LBC) | âœ… Tested | Fix #59: mario1-app1 remove NLB annotations |
| Chrome HTTP blocking | âœ… Tested | Fix #60: mario1-app1 use Brave/Firefox |
| GHA workflow location | âœ… Tested | Fix #61: test-green workflows at repo root |
| Docker ARM64 vs AMD64 | âœ… Tested | Fix #62: test-green --platform linux/amd64 |
| Git conflicts from auto-commits | âœ… Tested | Fix #63: mario1-app1 git pull --rebase |
| GitOps violation detection | âœ… Added | Guardrail #64: Pause before kubectl mutating |
| Multi-account pre-planning | âœ… Added | Guardrail #65: Account verification checklist |
| Preview before operating | âœ… Added | Guardrail #66: terraform plan, dry-run, diff |
| Generic AI fallback detection | âœ… Added | Guardrail #67: Skill gap detection |
| Brownfield-hub-spoke pattern | âœ… Tested | cdot8-home: argocd-nonprod-eks â†’ customer0-nonprod-eks |
| ECR race condition | âœ… Tested | Fix #68: cdot8-home GHA failed, re-triggered after ECR creation |
| Minimal human inputs | âœ… Validated | cdot8-home: Only 4 interactions (params, clusters, PAT, approval) |

---

### Skill Structure

```
Content Sections:
â”œâ”€â”€ ğŸ¤– Agent Behavior Rules
â”œâ”€â”€ Overview
â”œâ”€â”€ âš ï¸ Mandatory First Step
â”‚   â”œâ”€â”€ Required Inputs (4: identifier, region, env, pattern)
â”‚   â”œâ”€â”€ GitOps Architecture Patterns (NEW v2.1)
â”‚   â”‚   â”œâ”€â”€ Greenfield
â”‚   â”‚   â”œâ”€â”€ Brownfield-Embedded
â”‚   â”‚   â”œâ”€â”€ Brownfield-Hub-Spoke
â”‚   â”‚   â””â”€â”€ Brownfield-Cluster
â”‚   â”œâ”€â”€ Pattern Comparison Matrix (NEW v2.1)
â”‚   â”œâ”€â”€ Prompt Template
â”‚   â””â”€â”€ Verification Checklist
â”œâ”€â”€ Pattern-Specific Setup Guides (NEW v2.1)
â”œâ”€â”€ Naming Convention
â”œâ”€â”€ Resource Tagging Standard
â”œâ”€â”€ Mental Model (4-Layer Architecture)
â”œâ”€â”€ Template Directory Structure
â”œâ”€â”€ Pre-Configured Infrastructure Values
â”œâ”€â”€ Quick Start
â”œâ”€â”€ Opsera Workbench Auto-Generation (NEW v3.0)
â”œâ”€â”€ Layer-by-Layer Setup Guide
â”œâ”€â”€ Custom Domain Setup (GoDaddy/Route53/ACM)
â”œâ”€â”€ New AWS Account Full Setup
â”œâ”€â”€ Verified Fixes (67 items)
â”œâ”€â”€ Idempotent Patterns
â”œâ”€â”€ Pod Capacity Reference
â”œâ”€â”€ Troubleshooting Quick Reference
â”œâ”€â”€ Environment Matrix
â”œâ”€â”€ Supported Languages
â”œâ”€â”€ Cleanup Procedure
â”œâ”€â”€ Verified Working Endpoints
â”œâ”€â”€ Multi-Account Deployment Checklist
â”œâ”€â”€ Handoff Document Template
â”œâ”€â”€ Skills This Replaces (9 skills)
â””â”€â”€ Changelog & Improvements
```

---

### Skills This Skill Replaces

This unified skill consolidates and supersedes:
- `container-eks-gitops`
- `gitops-eks-deploy`
- `kustomize-app-deploy`
- `ecr-image-push`
- `route53-friendly-url`
- `opsera_cicd_container_aws`
- `terraform-eks-patterns`
- `idempotent-scripts`
- `k8s-debugging-guide`

**DO NOT use these skills if this skill is available.**

---

---

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                          â•‘
â•‘    â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                         â•‘
â•‘    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘                         â•‘
â•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                         â•‘
â•‘     â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•                         â•‘
â•‘                                                                               â•‘
â•‘                    â”â”â” UNIFIED DEVOPS PLATFORM â”â”â”                           â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Last verified: January 2026                                                  â•‘
â•‘  Skill version: 3.6.0                                                         â•‘
â•‘                                                                               â•‘
â•‘  New in 3.6.0 (cdot8-home learnings):                                         â•‘
â•‘  â€¢ ECR race condition fix (Fix #68) - create repos before git push           â•‘
â•‘  â€¢ Brownfield-hub-spoke pattern fully validated                              â•‘
â•‘  â€¢ GitHub Actions CI/CD pipeline tested end-to-end                           â•‘
â•‘  â€¢ Minimal human inputs validated (4 interactions for full deployment)       â•‘
â•‘  â€¢ ~10 minute deployment time achieved                                       â•‘
â•‘                                                                               â•‘
â•‘  Verified patterns:                                                           â•‘
â•‘  â€¢ Greenfield (new VPC, EKS clusters, private subnets)                       â•‘
â•‘  â€¢ Brownfield-embedded (existing cluster with ArgoCD)                        â•‘
â•‘  â€¢ Brownfield-hub-spoke (separate ArgoCD + workload clusters)                â•‘
â•‘  â€¢ Custom domain (GoDaddy + Route53 + ACM)                                   â•‘
â•‘  â€¢ Multi-region deployments (us-west-1, us-west-2, us-east-1)               â•‘
â•‘                                                                               â•‘
â•‘  Total verified fixes: 68                                                     â•‘
â•‘  Test coverage: 34/35 scenarios tested                                        â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          Powered by Opsera DevOps Platform
```
